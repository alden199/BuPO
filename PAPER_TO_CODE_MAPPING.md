# BuPO è®ºæ–‡ä¸ä»£ç æ˜ å°„æ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†è®ºæ–‡ã€ŠBottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policiesã€‹ä¸­çš„æ ¸å¿ƒæ¦‚å¿µåœ¨ä»£ç åº“ä¸­çš„å…·ä½“å®ç°ä½ç½®ã€‚

---

## ğŸ“„ è®ºæ–‡æ ¸å¿ƒæ¦‚å¿µæ€»è§ˆ

è®ºæ–‡æå‡ºäº†ä¸‰ä¸ªæ ¸å¿ƒè´¡çŒ®ï¼š
1. **Internal Policy åˆ†è§£**ï¼šå°†è¯­è¨€æ¨¡å‹ç­–ç•¥åˆ†è§£ä¸ºå†…éƒ¨å±‚ç­–ç•¥å’Œæ¨¡å—ç­–ç•¥
2. **Entropy åˆ†æ**ï¼šé€šè¿‡ç†µåˆ†ææ­ç¤ºä¸åŒæ¨¡å‹çš„æ¨ç†æ¨¡å¼
3. **BuPO ç®—æ³•**ï¼šè‡ªåº•å‘ä¸Šçš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•

---

## 1ï¸âƒ£ Internal Policy å®šä¹‰ä¸å®ç°

### è®ºæ–‡ä½ç½®ï¼šSection 3.1 "Definition of Internal Policy"

#### ğŸ“– è®ºæ–‡å…¬å¼

**Internal Layer Policy**ï¼ˆå…¬å¼ 6ï¼‰ï¼š
```
Ï€^l_Layer â‰¡ P^l_Layer = softmax(H^l E^T_u)
```

**Internal Modular Policy**ï¼ˆå…¬å¼ 7ï¼‰ï¼š
```
Ï€^l_ATTN = softmax(A^l E^T_u)
Ï€^l_FFN = softmax(F^l E^T_u)
```

#### ğŸ’» ä»£ç å®ç°ä½ç½®

**1. è‡ªå®šä¹‰æ¨¡å‹è¾“å‡ºç±»**
- **æ–‡ä»¶**: `verl/models/custom_model/modeling_qwen3.py`
- **è¡Œå·**: 88-120
- **ç±»å**: `CausalLMOutputWithPastNew`
- **å…³é”®ä»£ç **:
```python
class CausalLMOutputWithPastNew(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    past_key_values: Optional[UserDict[str, Cache]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    mid_layer_logits: Optional[UserDict[int, torch.FloatTensor]] = None  # å­˜å‚¨å†…éƒ¨å±‚ logits
```

**2. æå–å†…éƒ¨éšè—çŠ¶æ€**
- **æ–‡ä»¶**: `verl/models/custom_model/modeling_qwen3.py`
- **è¡Œå·**: 585-596
- **åŠŸèƒ½**: ä»æŒ‡å®šå±‚æå–éšè—çŠ¶æ€å¹¶è®¡ç®— logits
- **å…³é”®ä»£ç **:
```python
# æå–å†…éƒ¨å±‚éšè—çŠ¶æ€
internal_logits = {}
startk = int(self.config.internal_layer)
for i in range(startk, startk+1):
    # H^l E^T_u çš„å®ç°
    internal_logits[i] = self.lm_head(outputs.hidden_states[i+1][:, slice_indices, :])
output.mid_layer_logits = internal_logits
```

**3. å†…éƒ¨ç­–ç•¥å‰å‘ä¼ æ’­**
- **æ–‡ä»¶**: `verl/workers/actor/dp_actor.py`
- **è¡Œå·**: 357-555
- **æ–¹æ³•**: `_forward_micro_batch_layer_k()`
- **åŠŸèƒ½**: è®¡ç®—å†…éƒ¨å±‚ç­–ç•¥çš„ log probability å’Œ entropy
- **å…³é”®ä»£ç **ï¼ˆè¡Œ 460ï¼‰:
```python
# ä»å†…éƒ¨å±‚è·å– logits
logits_rmpad = output.mid_layer_logits[layer_k].squeeze(0)
logits_rmpad.div_(temperature)
```

**4. æ¨¡å‹é…ç½®ä¸­çš„å†…éƒ¨å±‚è®¾ç½®**
- **æ–‡ä»¶**: `verl/models/custom_model/configuration_qwen3.py`
- **é…ç½®é¡¹**: `internal_layer`
- **è¯´æ˜**: æŒ‡å®šè¦ä¼˜åŒ–çš„å†…éƒ¨å±‚ç´¢å¼•ï¼ˆä¾‹å¦‚ï¼šlayer 6ï¼‰

---

## 2ï¸âƒ£ Entropy è®¡ç®—ä¸åˆ†æ

### è®ºæ–‡ä½ç½®ï¼šSection 3.2 "Internal Policy Entropy Dynamics"

#### ğŸ“– è®ºæ–‡å…¬å¼

**Internal Policy Entropy**ï¼ˆå…¬å¼ 8ï¼‰ï¼š
```
H^l_Layer = -Î£ P^l_Layer,j Â· log(P^l_Layer,j)
```

**Entropy Change**ï¼ˆå…¬å¼ 9ï¼‰ï¼š
```
Î”H^l = H^l_Output - H^l_Input
```

#### ğŸ’» ä»£ç å®ç°ä½ç½®

**1. Entropy è®¡ç®—å‡½æ•°**
- **æ–‡ä»¶**: `verl/workers/actor/dp_actor.py`
- **ç›¸å…³è¡Œå·**:
  - è¡Œ 351: `entropy = verl_F.entropy_from_logits(logits)`
  - è¡Œ 482: `entropy_rmpad = self.compute_entropy_from_logits(logits_rmpad)`
  - è¡Œ 549: æ ‡å‡† entropy è®¡ç®—

**2. Entropy å¯è§†åŒ–åˆ†æ**
- **æ–‡ä»¶**: `visualization/plot_internal_entropy.py`
- **è¡Œå·**: 1-619ï¼ˆå®Œæ•´æ–‡ä»¶ï¼‰
- **ä¸»è¦ç±»**: `EntropyAnalyzer`
- **åŠŸèƒ½**:
  - è®¡ç®—æ¯å±‚çš„ entropyï¼ˆè¡Œ 40-65ï¼‰
  - è®¡ç®— entropy changeï¼ˆè®ºæ–‡ä¸­çš„ Î”Hï¼‰
  - å¯è§†åŒ– entropy åŠ¨æ€å˜åŒ–

**3. Hook æ³¨å†Œè·å–å†…éƒ¨çŠ¶æ€**
- **æ–‡ä»¶**: `visualization/plot_internal_entropy.py`
- **è¡Œå·**: 66-115
- **æ–¹æ³•**: `_register_hooks()`
- **åŠŸèƒ½**: æ³¨å†Œé’©å­å‡½æ•°è·å– ATTN å’Œ FFN çš„è¾“å…¥è¾“å‡º

**4. Entropy è®¡ç®—å®ç°**
- **æ–‡ä»¶**: `visualization/plot_internal_entropy.py`
- **è¡Œå·**: 200-250ï¼ˆå¤§çº¦ï¼‰
- **åŠŸèƒ½**: å¯¹æ¯ä¸ªæ¨¡å—ï¼ˆLayer/ATTN/FFNï¼‰è®¡ç®— entropy
- **å®ç°**: ä½¿ç”¨ `softmax(H^l E^T_u)` ç„¶åè®¡ç®— `-Î£ p log(p)`

---

## 3ï¸âƒ£ Internal Policy Optimization (InterGRPO)

### è®ºæ–‡ä½ç½®ï¼šSection 4 "Internal Policy Optimization"

#### ğŸ“– è®ºæ–‡å…¬å¼ï¼ˆå…¬å¼ 10ï¼‰

```
J_InterGRPO(Ï€Î¸, Ï€^l_Layer) = E[min{rÌ‚_i,t Ã‚_i,t, clip(rÌ‚_i,t, 1-Îµ, 1+Îµ)Ã‚_i,t}]

å…¶ä¸­: rÌ‚_i,t = Ï€^l_Layer(o_i,t|q,o_i,<t) / Ï€^l_Layer,old(o_i,t|q,o_i,<t)
```

#### ğŸ’» ä»£ç å®ç°ä½ç½®

**1. InterGRPO å®ç°çš„æ ¸å¿ƒé€»è¾‘**
- **æ–‡ä»¶**: `verl/workers/actor/dp_actor.py`
- **è¡Œå·**: 806-820ï¼ˆç¬¬ä¸€å¤„è°ƒç”¨ï¼‰
- **å…³é”®åˆ¤æ–­**:
```python
# BuPO æ¨¡å¼åˆ¤æ–­
if self.config.internal_policy_interative:
    if micro_batch.meta_info['global_steps'] <= self.config.iterative_steps:
        # Phase 1: ä¼˜åŒ–å†…éƒ¨å±‚ç­–ç•¥
        entropy, log_probs = self._forward_micro_batch_layer_k(
            model_inputs,
            temperature=temperature,
            calculate_entropy=calculate_entropy,
            layer_k=self.config.internal_layer
        )
    else:
        # Phase 2: ä¼˜åŒ–å®Œæ•´æ¨¡å‹ç­–ç•¥
        entropy, log_probs = self._forward_micro_batch(
            model_inputs,
            temperature=temperature,
            calculate_entropy=calculate_entropy
        )
```

**2. InterGRPO å®ç°çš„ç¬¬äºŒå¤„**
- **æ–‡ä»¶**: `verl/workers/actor/dp_actor.py`
- **è¡Œå·**: 906-920
- **è¯´æ˜**: ä¸ç¬¬ä¸€å¤„é€»è¾‘ç›¸åŒï¼Œå¤„ç†ä¸åŒçš„å‰å‘ä¼ æ’­è·¯å¾„

**3. æ¢¯åº¦æµæ§åˆ¶**
- **æ–‡ä»¶**: Appendix A.5ï¼ˆè®ºæ–‡ï¼‰å¯¹åº”ä»£ç åœ¨è®­ç»ƒæ—¶è‡ªåŠ¨å¤„ç†
- **åŸç†**: ç”±äºæ®‹å·®è¿æ¥ï¼Œä¼˜åŒ– Ï€^l_Layer æ—¶åªæ›´æ–° layers 0 åˆ° l çš„å‚æ•°
- **å…¬å¼ï¼ˆè®ºæ–‡å…¬å¼ 16ï¼‰**:
```
âˆ‚J_InterGRPO/âˆ‚Î¸_k = {
    æ¢¯åº¦è®¡ç®—, if k â‰¤ l
    0,        if k > l
}
```

---

## 4ï¸âƒ£ Bottom-up Policy Optimization (BuPO) ç®—æ³•

### è®ºæ–‡ä½ç½®ï¼šSection 5 "Bottom-up Policy Optimization" + Algorithm 1

#### ğŸ“– è®ºæ–‡å…¬å¼ï¼ˆå…¬å¼ 11ï¼‰

```
J_BuPO(Ï€Î¸, Ï€^l_Layer) = {
    J_InterGRPO(Ï€Î¸, Ï€^l_Layer), if s_cur â‰¤ s_inter
    J_GRPO(Ï€Î¸),                  if s_cur > s_inter
}
```

#### ğŸ’» ä»£ç å®ç°ä½ç½®

**1. BuPO é…ç½®å‚æ•°**
- **æ–‡ä»¶**: `verl/workers/config/actor.py`
- **è¡Œå·**: 225-240
- **é…ç½®ç±»**: `FSDPActorConfig`
- **å…³é”®å‚æ•°**:
```python
internal_policy_interative: bool = False  # å¯ç”¨ BuPO
iterative_steps: int = 30                 # s_inter: å†…éƒ¨ç­–ç•¥ä¼˜åŒ–æ­¥æ•°
internal_layer: int = 6                   # l: è¦ä¼˜åŒ–çš„å±‚ç´¢å¼•
```

**2. BuPO é…ç½®æ–‡ä»¶**
- **æ–‡ä»¶**: `verl/trainer/config/actor/dp_actor.yaml`
- **è¡Œå·**: 87-89
```yaml
internal_policy_interative: False
iterative_steps: 30
```

**3. BuPO ä¸»è®­ç»ƒå…¥å£**
- **æ–‡ä»¶**: `verl/trainer/main_ppo.py`
- **è¯´æ˜**: ä½¿ç”¨ Hydra é…ç½®ç³»ç»Ÿï¼Œé€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
- **è°ƒç”¨ä½ç½®**: è®­ç»ƒè„šæœ¬é€šè¿‡è¯¥æ–‡ä»¶å¯åŠ¨ PPO è®­ç»ƒ

**4. è‡ªå®šä¹‰æ¨¡å‹åŠ è½½**
- **æ–‡ä»¶**: `verl/workers/fsdp_workers.py`
- **è¡Œå·**: 249-256
- **åŠŸèƒ½**: å½“å¯ç”¨ BuPO æ—¶ï¼Œæ›¿æ¢æ ‡å‡† transformers æ¨¡å‹ä¸ºè‡ªå®šä¹‰æ¨¡å‹
- **å…³é”®ä»£ç **:
```python
if hasattr(self.config.actor, "internal_policy_interative") and \
   self.config.actor.internal_policy_interative:
    from verl.models.custom_model import modeling_qwen2 as custom_modeling_qwen2
    from verl.models.custom_model import modeling_qwen3 as custom_modeling_qwen3
    from verl.models.custom_model import modeling_llama as custom_modeling_llama
    sys.modules["transformers.models.qwen2.modeling_qwen2"] = custom_modeling_qwen2
    sys.modules["transformers.models.qwen3.modeling_qwen3"] = custom_modeling_qwen3
    sys.modules["transformers.models.llama.modeling_llama"] = custom_modeling_llama
```

---

## 5ï¸âƒ£ è®­ç»ƒè„šæœ¬ä¸å‘½ä»¤

### è®ºæ–‡ä½ç½®ï¼šSection 5.1 "Main Results" - Training Setup

#### ğŸ’» ä»£ç å®ç°ä½ç½®

**1. BuPO è®­ç»ƒè„šæœ¬ (Qwen)**
- **æ–‡ä»¶**: `run_code/BuPO_qwen3.sh`
- **å…³é”®å‚æ•°**ï¼ˆè¡Œ 48-51, 102-104ï¼‰:
```bash
k=5                                    # å†…éƒ¨å±‚ç´¢å¼•
iterative_steps=30                     # Phase 1 çš„è®­ç»ƒæ­¥æ•°
prompt_template_type="qwen3_no_thinking"
experiment_name="modelname_bupo_deepmath5k_${k}layerpolicy_iterstep${iterative_steps}..."

# Hydra å‚æ•°è¦†ç›–
actor_rollout_ref.actor.internal_policy_interative=True
actor_rollout_ref.actor.internal_layer=${k}
actor_rollout_ref.actor.iterative_steps=${iterative_steps}
actor_rollout_ref.model.override_config.internal_layer=${k}
```

**2. BuPO è®­ç»ƒè„šæœ¬ (Llama)**
- **æ–‡ä»¶**: `run_code/BuPO_llama.sh`
- **è¯´æ˜**: ç»“æ„ä¸ Qwen ç‰ˆæœ¬ç›¸åŒï¼Œå‚æ•°ç•¥æœ‰ä¸åŒ

**3. GRPO åŸºçº¿è„šæœ¬**
- **æ–‡ä»¶**: `run_code/GRPO_qwen3.sh`
- **è¯´æ˜**: æ ‡å‡† GRPOï¼Œä¸å¯ç”¨ `internal_policy_interative`

**4. GRPO åŸºçº¿è„šæœ¬ (Llama)**
- **æ–‡ä»¶**: `run_code/GRPO_llama.sh`

---

## 6ï¸âƒ£ è¯„ä¼°ä¸å¯è§†åŒ–

### è®ºæ–‡ä½ç½®ï¼šSection 5.1 "Main Results" - Evaluation Setup

#### ğŸ’» ä»£ç å®ç°ä½ç½®

**1. è¯„ä¼°è„šæœ¬**
- **æ–‡ä»¶**: `scripts/run_eval.sh`
- **è¡Œå·**: 1-38
- **åŠŸèƒ½**: åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šç”Ÿæˆé¢„æµ‹
- **å…³é”®é…ç½®**:
```bash
model=Qwen3-4B                        # æ¨¡å‹è·¯å¾„
dataset=math500                        # æ•°æ®é›†é€‰æ‹©
tp_size=2                             # Tensor å¹¶è¡Œå¤§å°
n_samples=1                           # æ¯ä¸ª prompt çš„æ ·æœ¬æ•°
```

**2. å¯è§†åŒ–è„šæœ¬**
- **æ–‡ä»¶**: `visualization/plot_internal_entropy.py`
- **è¡Œå·**: å®Œæ•´æ–‡ä»¶ï¼ˆ619è¡Œï¼‰
- **ä¸»è¦åŠŸèƒ½**:
  - åŠ è½½æ¨¡å‹å¹¶æ³¨å†Œ hooksï¼ˆè¡Œ 40-115ï¼‰
  - åˆ†æ entropy åŠ¨æ€ï¼ˆè¡Œ 200-400ï¼‰
  - ç»˜åˆ¶ Figure 2, 3, 4ï¼ˆè®ºæ–‡ä¸­çš„å›¾ï¼‰
  - ç”Ÿæˆ Entropy Change å¯è§†åŒ–

**3. è®ºæ–‡ Figure å¯¹åº”**
- **Figure 2**: "Continuous Entropy Flow Through Layers"
  - ä»£ç ä½ç½®: `visualization/plot_internal_entropy.py` ä¸­çš„ç»˜å›¾å‡½æ•°
  - æ•°æ®: Layer I/O, ATTN, FFN çš„ entropy

- **Figure 3**: "Entropy Change Dynamics"
  - ä»£ç ä½ç½®: è®¡ç®— Î”H^l_ATTN å’Œ Î”H^l_FFN
  - å…¬å¼: Î”H^l = H^l_Output - H^l_Input

---

## 7ï¸âƒ£ æ•°æ®æµä¸è®­ç»ƒæµç¨‹

### è®ºæ–‡ä½ç½®ï¼šSection 2.2 + Section 5

#### ğŸ“Š å®Œæ•´æ•°æ®æµ

```
è¾“å…¥æ•°æ® (data/)
    â”œâ”€â”€ deepmath-5k.parquet          # è®­ç»ƒé›†
    â”œâ”€â”€ aime_2024.parquet            # éªŒè¯é›†
    â”œâ”€â”€ aime_2025.parquet
    â”œâ”€â”€ amc2023.parquet
    â””â”€â”€ math500.parquet

                    â†“

verl/trainer/main_ppo.py             # ä¸»è®­ç»ƒå…¥å£
                    â†“

verl/workers/fsdp_workers.py         # åŠ è½½è‡ªå®šä¹‰æ¨¡å‹
    (è¡Œ 249-256)
                    â†“

verl/models/custom_model/            # è‡ªå®šä¹‰æ¨¡å‹
    â”œâ”€â”€ modeling_qwen3.py            # è¿”å› hidden_states å’Œ mid_layer_logits
    â”œâ”€â”€ modeling_llama.py
    â””â”€â”€ configuration_*.py

                    â†“

verl/workers/actor/dp_actor.py       # Actor å®ç°
    â”œâ”€â”€ _forward_micro_batch_layer_k() (è¡Œ 357)  # å†…éƒ¨å±‚å‰å‘ä¼ æ’­
    â””â”€â”€ ä¸¤é˜¶æ®µåˆ‡æ¢é€»è¾‘ (è¡Œ 806, 906)

                    â†“

è®­ç»ƒè¾“å‡º
    â””â”€â”€ checkpoints/BuPO/{experiment_name}/
```

#### ğŸ”„ BuPO ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹

**Phase 1: Internal Policy Optimization** (æ­¥éª¤ 1 åˆ° `iterative_steps`)
1. å‰å‘ä¼ æ’­åˆ°ç¬¬ k å±‚
2. è®¡ç®— Ï€^k_Layer çš„ log probs
3. è®¡ç®— importance ratio: rÌ‚ = Ï€^k / Ï€^k_old
4. ä½¿ç”¨ PPO loss æ›´æ–° layers 0 åˆ° k
5. æ¢¯åº¦è‡ªåŠ¨åœæ­¢åœ¨ç¬¬ k å±‚ï¼ˆæ®‹å·®è¿æ¥ç‰¹æ€§ï¼‰

**Phase 2: Full Model Optimization** (æ­¥éª¤ > `iterative_steps`)
1. æ ‡å‡† GRPO/PPO å‰å‘ä¼ æ’­
2. ä½¿ç”¨å®Œæ•´æ¨¡å‹ç­–ç•¥ Ï€Î¸
3. æ›´æ–°æ‰€æœ‰å±‚å‚æ•°

---

## 8ï¸âƒ£ å…³é”®è¶…å‚æ•°å¯¹åº”

### è®ºæ–‡ä½ç½®ï¼šTable 5 (Appendix A.6)

| è®ºæ–‡å‚æ•° | ä»£ç ä½ç½® | é»˜è®¤å€¼ | è¯´æ˜ |
|---------|---------|--------|------|
| Policy learning rate | `actor_rollout_ref.actor.optim.lr` | 1e-6 | ç­–ç•¥ç½‘ç»œå­¦ä¹ ç‡ |
| Training batch size | `data.train_batch_size` | 128 prompts | æ¯æ‰¹ prompt æ•°é‡ |
| Samples per prompt | `actor_rollout_ref.rollout.n` | 8 | æ¯ä¸ª prompt ç”Ÿæˆçš„å“åº”æ•° |
| Mini-batch size | `actor_rollout_ref.actor.ppo_mini_batch_size` | 32 | PPO mini-batch |
| Max prompt length | `data.max_prompt_length` | 1024 tokens | æœ€å¤§ prompt é•¿åº¦ |
| Max response length | `data.max_response_length` | 7168 (Qwen) / 3072 (Llama) | æœ€å¤§å“åº”é•¿åº¦ |
| Rollout temperature | `actor_rollout_ref.rollout.temperature` | 1.0 | é‡‡æ ·æ¸©åº¦ |
| Clip range Îµ | `actor_rollout_ref.actor.clip_ratio_low/high` | 0.2 | PPO è£å‰ªèŒƒå›´ |
| **BuPO specific** | | | |
| Internal layer k | `actor_rollout_ref.actor.internal_layer` | 5 (Qwen3-4B), 6 (Qwen3-8B) | ä¼˜åŒ–çš„å†…éƒ¨å±‚ |
| Iterative steps | `actor_rollout_ref.actor.iterative_steps` | 20-30 | Phase 1 æ­¥æ•° |

---

## 9ï¸âƒ£ å®éªŒç»“æœå¯¹åº”

### è®ºæ–‡ä½ç½®ï¼šTable 1 - Main Results

#### ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡è®¡ç®—

**Avg@K**ï¼ˆè®ºæ–‡ä¸­ä½¿ç”¨ï¼‰:
- **å®ç°**: å¯¹æ¯ä¸ªé—®é¢˜ç”Ÿæˆ K ä¸ªå“åº”ï¼Œè®¡ç®— Pass@1 çš„å¹³å‡å€¼
- **ä»£ç **: è¯„ä¼°é€»è¾‘åœ¨ vLLM ä¸­å¤„ç†

**Pass@K**ï¼ˆè®ºæ–‡ Figure 7ï¼‰:
- **å…¬å¼**: `Pass@K = E[1 - C(n-c, K) / C(n, K)]`
- **å‚æ•°**: n=300ï¼ˆæ€»ç”Ÿæˆæ•°ï¼‰ï¼Œcï¼ˆæ­£ç¡®æ•°ï¼‰ï¼ŒKï¼ˆé‡‡æ ·æ•°ï¼‰
- **èŒƒå›´**: K âˆˆ {1, 4, 16, 64, 256}

---

## ğŸ”Ÿ è®ºæ–‡ç®—æ³•ä¼ªä»£ç å¯¹åº”

### Algorithm 1: Bottom-up Policy Optimization (BuPO)

| ç®—æ³•è¡Œ | è®ºæ–‡æè¿° | ä»£ç ä½ç½® | è¯´æ˜ |
|--------|---------|---------|------|
| Line 1 | Initialize scur â† 0 | è®­ç»ƒå¾ªç¯ä¸­çš„å…¨å±€æ­¥æ•° | ç”±è®­ç»ƒæ¡†æ¶ç®¡ç† |
| Line 3 | Sample batch q ~ Q | `data.train_files` | ä»è®­ç»ƒé›†é‡‡æ · |
| Line 4 | Generate G outputs | `actor_rollout_ref.rollout.n=8` | æ¯ä¸ª prompt ç”Ÿæˆ 8 ä¸ªå“åº” |
| Line 5 | Compute rewards and advantages | PPO æ ‡å‡†æµç¨‹ | GRPO ä¼˜åŠ¿ä¼°è®¡ |
| Line 6 | if scur â‰¤ sinter | `verl/workers/actor/dp_actor.py:807` | åˆ¤æ–­å½“å‰æ­¥æ•° |
| Line 7-9 | Phase 1: Optimize Ï€^l_Layer | `_forward_micro_batch_layer_k()` | å†…éƒ¨ç­–ç•¥ä¼˜åŒ– |
| Line 11-12 | Phase 2: Optimize Ï€Î¸ | `_forward_micro_batch()` | å®Œæ•´æ¨¡å‹ä¼˜åŒ– |
| Line 14 | Update parameters | æ ‡å‡† PyTorch ä¼˜åŒ–å™¨ | AdamWï¼Œlr=1e-6 |

---

## ğŸ“š è¡¥å……è¯´æ˜

### ä»£ç ä¸­çš„å‘½åçº¦å®š

1. **Layer ç´¢å¼•**:
   - è®ºæ–‡ä¸­ï¼šLayer l âˆˆ [1, L]
   - ä»£ç ä¸­ï¼šLayer index âˆˆ [0, L-1]
   - **æ³¨æ„**: ä»£ç ä¸­çš„ layer 0 å¯¹åº”è®ºæ–‡ä¸­çš„ layer 1

2. **Hidden states**:
   - `H^l` (è®ºæ–‡) = `outputs.hidden_states[l+1]` (ä»£ç )
   - å› ä¸º `hidden_states[0]` æ˜¯ embedding

3. **æ¨¡å—å‘½å**:
   - ATTN (è®ºæ–‡) = self_attn (ä»£ç )
   - FFN (è®ºæ–‡) = mlp (ä»£ç )

### å…³é”®æ–‡ä»¶æ€»ç»“

| è®ºæ–‡æ¦‚å¿µ | ä¸»è¦å®ç°æ–‡ä»¶ | æ ¸å¿ƒè¡Œå· |
|---------|------------|---------|
| Internal Policy å®šä¹‰ | `verl/models/custom_model/modeling_qwen3.py` | 585-596 |
| Entropy è®¡ç®— | `verl/workers/actor/dp_actor.py` | 351, 482, 549 |
| BuPO ç®—æ³• | `verl/workers/actor/dp_actor.py` | 806-820, 906-920 |
| è®­ç»ƒé…ç½® | `run_code/BuPO_qwen3.sh` | 48-51, 102-104 |
| å¯è§†åŒ–åˆ†æ | `visualization/plot_internal_entropy.py` | å®Œæ•´æ–‡ä»¶ |
| è¯„ä¼°è„šæœ¬ | `scripts/run_eval.sh` | å®Œæ•´æ–‡ä»¶ |

---

## âœ… å¿«é€ŸæŸ¥æ‰¾æŒ‡å—

**æƒ³è¦æ‰¾è®ºæ–‡ä¸­çš„æŸä¸ªæ¦‚å¿µï¼Ÿ**

- **å…¬å¼ 6 (Internal Layer Policy)** â†’ `verl/models/custom_model/modeling_qwen3.py:595`
- **å…¬å¼ 8 (Entropy è®¡ç®—)** â†’ `verl/workers/actor/dp_actor.py:351, 482`
- **å…¬å¼ 10 (InterGRPO)** â†’ `verl/workers/actor/dp_actor.py:357-555`
- **å…¬å¼ 11 (BuPO)** â†’ `verl/workers/actor/dp_actor.py:806-820`
- **Algorithm 1** â†’ `verl/workers/actor/dp_actor.py` + è®­ç»ƒè„šæœ¬
- **Figure 2 (Entropy Flow)** â†’ `visualization/plot_internal_entropy.py`
- **Figure 3 (Entropy Change)** â†’ `visualization/plot_internal_entropy.py`
- **Table 1 (Results)** â†’ é€šè¿‡ `scripts/run_eval.sh` ç”Ÿæˆ

---

## ğŸ¯ è¿è¡Œå®Œæ•´æµç¨‹

### 1. è®­ç»ƒ BuPO æ¨¡å‹
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export MODEL_PATH="your/model/path"
export DATA_PATH="your/data/path"

# è¿è¡Œ BuPO è®­ç»ƒ
bash run_code/BuPO_qwen3.sh
```

### 2. è¯„ä¼°æ¨¡å‹
```bash
# ç”Ÿæˆé¢„æµ‹
bash scripts/run_eval.sh
```

### 3. å¯è§†åŒ–åˆ†æ
```bash
# ç»˜åˆ¶ Internal Entropy å›¾
python visualization/plot_internal_entropy.py
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2026-01-06
**å¯¹åº”è®ºæ–‡**: arXiv:2512.19673v1

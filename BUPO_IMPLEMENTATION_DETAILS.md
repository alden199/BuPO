# BuPO ç®—æ³•è¯¦ç»†å®ç°è§£æ

æœ¬æ–‡æ¡£é€è¡Œè§£é‡Š BuPOï¼ˆBottom-up Policy Optimizationï¼‰ç®—æ³•çš„å®Œæ•´å®ç°ç»†èŠ‚ï¼Œé¢å‘ä¸ç†Ÿæ‚‰ verl æ¡†æ¶çš„è¯»è€…ã€‚

---

## ğŸ“š ç›®å½•

1. [verl æ¡†æ¶åŸºç¡€æ¦‚å¿µ](#1-verl-æ¡†æ¶åŸºç¡€æ¦‚å¿µ)
2. [BuPO è®­ç»ƒæµç¨‹æ€»è§ˆ](#2-bupo-è®­ç»ƒæµç¨‹æ€»è§ˆ)
3. [è®­ç»ƒå…¥å£ï¼šmain_ppo.py](#3-è®­ç»ƒå…¥å£mainppopy)
4. [Actor æ ¸å¿ƒå®ç°ï¼šdp_actor.py](#4-actor-æ ¸å¿ƒå®ç°dpactorpy)
5. [BuPO ä¸¤é˜¶æ®µåˆ‡æ¢é€»è¾‘](#5-bupo-ä¸¤é˜¶æ®µåˆ‡æ¢é€»è¾‘)
6. [å†…éƒ¨å±‚å‰å‘ä¼ æ’­](#6-å†…éƒ¨å±‚å‰å‘ä¼ æ’­)
7. [è‡ªå®šä¹‰æ¨¡å‹å®ç°](#7-è‡ªå®šä¹‰æ¨¡å‹å®ç°)
8. [Loss è®¡ç®—ä¸åå‘ä¼ æ’­](#8-loss-è®¡ç®—ä¸åå‘ä¼ æ’­)
9. [å®Œæ•´è®­ç»ƒå¾ªç¯](#9-å®Œæ•´è®­ç»ƒå¾ªç¯)
10. [é…ç½®å‚æ•°è¯¦è§£](#10-é…ç½®å‚æ•°è¯¦è§£)

---

## 1. verl æ¡†æ¶åŸºç¡€æ¦‚å¿µ

### 1.1 verl æ˜¯ä»€ä¹ˆï¼Ÿ

**verl (Volcano Engine Reinforcement Learning)** æ˜¯å­—èŠ‚è·³åŠ¨å¼€æºçš„å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œä¸“é—¨ä¸º LLM çš„ RL è®­ç»ƒè®¾è®¡ã€‚

### 1.2 æ ¸å¿ƒç»„ä»¶

```
verl æ¡†æ¶æ¶æ„
â”œâ”€â”€ Trainer (è®­ç»ƒå™¨)
â”‚   â”œâ”€â”€ main_ppo.py           # ä¸»è®­ç»ƒå…¥å£
â”‚   â””â”€â”€ RayPPOTrainer         # Ray åˆ†å¸ƒå¼è®­ç»ƒå™¨
â”‚
â”œâ”€â”€ Workers (å·¥ä½œèŠ‚ç‚¹)
â”‚   â”œâ”€â”€ Actor (ç­–ç•¥ç½‘ç»œ)       # ç”ŸæˆåŠ¨ä½œå¹¶æ›´æ–°ç­–ç•¥
â”‚   â”œâ”€â”€ Critic (ä»·å€¼ç½‘ç»œ)      # ä¼°è®¡çŠ¶æ€ä»·å€¼ï¼ˆPPO éœ€è¦ï¼‰
â”‚   â”œâ”€â”€ Rollout (æ¨ç†å¼•æ“)     # ä½¿ç”¨ vLLM ç”Ÿæˆå“åº”
â”‚   â””â”€â”€ Reference (å‚è€ƒç­–ç•¥)   # ç”¨äº KL æƒ©ç½š
â”‚
â”œâ”€â”€ Models (æ¨¡å‹å±‚)
â”‚   â”œâ”€â”€ custom_model/         # è‡ªå®šä¹‰æ¨¡å‹ï¼ˆæ”¯æŒå†…éƒ¨å±‚è¾“å‡ºï¼‰
â”‚   â””â”€â”€ transformers æ ‡å‡†æ¨¡å‹
â”‚
â””â”€â”€ Data (æ•°æ®å±‚)
    â””â”€â”€ DataProto              # ç»Ÿä¸€æ•°æ®æ ¼å¼
```

### 1.3 å…³é”®æ¦‚å¿µ

#### 1.3.1 DataProto

verl ä½¿ç”¨ `DataProto` ç»Ÿä¸€ç®¡ç†æ•°æ®ï¼ŒåŒ…å«ï¼š
- **batch**: å¼ é‡æ•°æ®ï¼ˆinput_ids, attention_mask ç­‰ï¼‰
- **non_tensor_batch**: éå¼ é‡æ•°æ®ï¼ˆå›¾åƒç­‰ï¼‰
- **meta_info**: å…ƒä¿¡æ¯ï¼ˆglobal_steps, temperature ç­‰ï¼‰

#### 1.3.2 Actor vs Rollout

- **Rollout**: ä½¿ç”¨ vLLM å¿«é€Ÿç”Ÿæˆå¤šä¸ªå“åº”ï¼ˆæ¨ç†ä¼˜åŒ–ï¼‰
- **Actor**: ä½¿ç”¨å®Œæ•´æ¨¡å‹è®¡ç®— log_probs å’Œæ¢¯åº¦ï¼ˆè®­ç»ƒä¼˜åŒ–ï¼‰

#### 1.3.3 FSDP (Fully Sharded Data Parallel)

PyTorch çš„åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼Œå°†æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ°å¤šä¸ª GPUã€‚

---

## 2. BuPO è®­ç»ƒæµç¨‹æ€»è§ˆ

### 2.1 æ•´ä½“æµç¨‹å›¾

```
ç”¨æˆ·å¯åŠ¨è®­ç»ƒ
    â†“
bash run_code/BuPO_qwen3.sh
    â†“
python -m verl.trainer.main_ppo (Hydra é…ç½®)
    â†“
main() å‡½æ•° â†’ run_ppo()
    â†“
åˆå§‹åŒ– Ray é›†ç¾¤
    â†“
åˆ›å»º TaskRunner (è¿œç¨‹æ‰§è¡Œ)
    â†“
TaskRunner.run()
    â†“
åˆ›å»º RayPPOTrainer
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ä¸»è®­ç»ƒå¾ªç¯ (æ¯ä¸ª step)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Rollout: ç”Ÿæˆå“åº” (vLLM)         â”‚
â”‚  2. è®¡ç®— Reward                      â”‚
â”‚  3. è®¡ç®— Advantage (GRPO)           â”‚
â”‚  4. Actor Update (BuPO æ ¸å¿ƒ)        â”‚
â”‚     â”œâ”€ åˆ¤æ–­å½“å‰æ­¥æ•°                  â”‚
â”‚     â”œâ”€ Phase 1: ä¼˜åŒ–å†…éƒ¨å±‚ç­–ç•¥       â”‚
â”‚     â””â”€ Phase 2: ä¼˜åŒ–å®Œæ•´æ¨¡å‹         â”‚
â”‚  5. Reference Policy Update         â”‚
â”‚  6. ä¿å­˜ Checkpoint                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 BuPO ç‰¹æœ‰çš„è®­ç»ƒé˜¶æ®µ

**Phase 1: Internal Policy Optimization**
- è®­ç»ƒæ­¥æ•°: 1 åˆ° `iterative_steps`ï¼ˆä¾‹å¦‚ 30ï¼‰
- ä¼˜åŒ–ç›®æ ‡: Ï€^k_Layerï¼ˆç¬¬ k å±‚çš„å†…éƒ¨ç­–ç•¥ï¼‰
- æ›´æ–°èŒƒå›´: åªæ›´æ–° layers 0 åˆ° k

**Phase 2: Full Model Optimization**
- è®­ç»ƒæ­¥æ•°: `iterative_steps + 1` åˆ° `total_steps`
- ä¼˜åŒ–ç›®æ ‡: Ï€Î¸ï¼ˆå®Œæ•´æ¨¡å‹ç­–ç•¥ï¼‰
- æ›´æ–°èŒƒå›´: æ›´æ–°æ‰€æœ‰å±‚

---

## 3. è®­ç»ƒå…¥å£ï¼šmain_ppo.py

### 3.1 æ–‡ä»¶ä½ç½®
```
verl/trainer/main_ppo.py
```

### 3.2 main() å‡½æ•°ï¼ˆç¬¬ 35-42 è¡Œï¼‰

```python
@hydra.main(config_path="config", config_name="ppo_trainer", version_base=None)
def main(config):
    """Main entry point for PPO training with Hydra configuration management.

    Args:
        config_dict: Hydra configuration dictionary containing training parameters.
    """
    run_ppo(config)
```

**é€è¡Œè§£æ**:

**ç¬¬ 35 è¡Œ**: `@hydra.main(...)`
- **Hydra è£…é¥°å™¨**: è‡ªåŠ¨åŠ è½½å’Œç®¡ç†é…ç½®æ–‡ä»¶
- `config_path="config"`: é…ç½®æ–‡ä»¶æ‰€åœ¨ç›®å½•
- `config_name="ppo_trainer"`: ä¸»é…ç½®æ–‡ä»¶å
- **ä½œç”¨**: å°† YAML é…ç½®å’Œå‘½ä»¤è¡Œå‚æ•°åˆå¹¶æˆ `config` å¯¹è±¡

**ç¤ºä¾‹**: å½“ä½ è¿è¡Œè®­ç»ƒè„šæœ¬æ—¶
```bash
python -m verl.trainer.main_ppo \
    actor_rollout_ref.actor.internal_policy_interative=True \
    actor_rollout_ref.actor.internal_layer=5
```
Hydra ä¼šï¼š
1. åŠ è½½ `config/ppo_trainer.yaml`
2. ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
3. ç”Ÿæˆæœ€ç»ˆçš„ `config` å¯¹è±¡

### 3.3 run_ppo() å‡½æ•°ï¼ˆç¬¬ 46-91 è¡Œï¼‰

```python
def run_ppo(config) -> None:
    """Initialize Ray cluster and run distributed PPO training process."""

    # ç¬¬ 55-66 è¡Œ: åˆå§‹åŒ– Ray
    if not ray.is_initialized():
        default_runtime_env = get_ppo_ray_runtime_env()
        ray_init_kwargs = config.ray_kwargs.get("ray_init", {})
        runtime_env_kwargs = ray_init_kwargs.get("runtime_env", {})
        runtime_env = OmegaConf.merge(default_runtime_env, runtime_env_kwargs)
        ray_init_kwargs = OmegaConf.create({**ray_init_kwargs, "runtime_env": runtime_env})
        print(f"ray init kwargs: {ray_init_kwargs}")
        ray.init(**OmegaConf.to_container(ray_init_kwargs))
```

**é€è¡Œè§£æ**:

**ç¬¬ 55 è¡Œ**: `if not ray.is_initialized():`
- **æ£€æŸ¥**: Ray æ˜¯å¦å·²ç»åˆå§‹åŒ–ï¼ˆé¿å…é‡å¤åˆå§‹åŒ–ï¼‰

**ç¬¬ 60 è¡Œ**: `default_runtime_env = get_ppo_ray_runtime_env()`
- **è·å–é»˜è®¤è¿è¡Œç¯å¢ƒ**: è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆTOKENIZERS_PARALLELISM, NCCL_DEBUG ç­‰ï¼‰

**ç¬¬ 61-64 è¡Œ**: åˆå¹¶é…ç½®
- å°†ç”¨æˆ·è‡ªå®šä¹‰çš„ Ray é…ç½®å’Œé»˜è®¤é…ç½®åˆå¹¶

**ç¬¬ 66 è¡Œ**: `ray.init(...)`
- **åˆå§‹åŒ– Ray é›†ç¾¤**:
  - å•æœºå¤šå¡: Ray ç®¡ç†æœ¬åœ° GPU
  - å¤šæœºå¤šå¡: Ray è¿æ¥åˆ°è¿œç¨‹é›†ç¾¤

```python
    # ç¬¬ 84-85 è¡Œ: åˆ›å»ºè¿œç¨‹ä»»åŠ¡æ‰§è¡Œå™¨
    runner = TaskRunner.remote()
    ray.get(runner.run.remote(config))
```

**é€è¡Œè§£æ**:

**ç¬¬ 84 è¡Œ**: `runner = TaskRunner.remote()`
- **åˆ›å»º Ray Actor**: `TaskRunner` ç±»è¢« `@ray.remote` è£…é¥°
- **remote()**: åœ¨ Ray é›†ç¾¤ä¸­åˆ›å»ºä¸€ä¸ªè¿œç¨‹å®ä¾‹
- **ä½œç”¨**: ä»»åŠ¡ä¼šåœ¨åˆ†é…çš„ CPU æ ¸å¿ƒä¸Šè¿è¡Œï¼ˆä¸å ç”¨ GPUï¼‰

**ç¬¬ 85 è¡Œ**: `ray.get(runner.run.remote(config))`
- `runner.run.remote(config)`: è¿œç¨‹è°ƒç”¨ `run` æ–¹æ³•ï¼ˆå¼‚æ­¥ï¼‰
- `ray.get(...)`: ç­‰å¾…è¿œç¨‹è°ƒç”¨å®Œæˆå¹¶è·å–è¿”å›å€¼
- **ä½œç”¨**: é˜»å¡ä¸»è¿›ç¨‹ç›´åˆ°è®­ç»ƒå®Œæˆ

### 3.4 TaskRunner ç±»ï¼ˆç¬¬ 94-100+ è¡Œï¼‰

```python
@ray.remote(num_cpus=1)
class TaskRunner:
    """Ray remote class for executing distributed PPO training tasks."""

    def run(self, config):
        # ... åˆ›å»º RayPPOTrainer å¹¶å¯åŠ¨è®­ç»ƒ ...
```

**å…³é”®ç‚¹**:
- `@ray.remote(num_cpus=1)`: åˆ†é… 1 ä¸ª CPU æ ¸å¿ƒ
- `run()` æ–¹æ³•å†…éƒ¨åˆ›å»º `RayPPOTrainer` å¹¶è°ƒç”¨å…¶ `train()` æ–¹æ³•

---

## 4. Actor æ ¸å¿ƒå®ç°ï¼šdp_actor.py

### 4.1 æ–‡ä»¶ä½ç½®
```
verl/workers/actor/dp_actor.py
æ€»è¡Œæ•°: 975 è¡Œ
```

### 4.2 DataParallelPPOActor ç±»

è¿™æ˜¯ BuPO çš„æ ¸å¿ƒå®ç°ï¼ŒåŒ…å«ï¼š
- ç­–ç•¥ç½‘ç»œçš„å‰å‘ä¼ æ’­
- Loss è®¡ç®—
- æ¢¯åº¦æ›´æ–°
- **BuPO ç‰¹æœ‰çš„ä¸¤é˜¶æ®µåˆ‡æ¢é€»è¾‘**

### 4.3 ç±»åˆå§‹åŒ–ï¼ˆç¬¬ 52-150 è¡Œï¼Œå¤§è‡´ï¼‰

```python
class DataParallelPPOActor(BasePPOActor):
    def __init__(
        self,
        config: ActorConfig,
        actor_module: nn.Module,
        actor_optimizer,
        actor_scheduler,
    ):
        self.config = config
        self.actor_module = actor_module  # FSDP åŒ…è£…çš„æ¨¡å‹
        self.actor_optimizer = actor_optimizer
        self.actor_scheduler = actor_scheduler

        # BuPO ç›¸å…³é…ç½®
        # config.internal_policy_interative: æ˜¯å¦å¯ç”¨ BuPO
        # config.internal_layer: ä¼˜åŒ–å“ªä¸€å±‚ï¼ˆä¾‹å¦‚ 5ï¼‰
        # config.iterative_steps: Phase 1 çš„æ­¥æ•°ï¼ˆä¾‹å¦‚ 30ï¼‰
```

**å…³é”®å±æ€§**:
- `self.config`: Actor é…ç½®ï¼ˆåŒ…å« BuPO å‚æ•°ï¼‰
- `self.actor_module`: ç»è¿‡ FSDP åŒ…è£…çš„æ¨¡å‹
- `self.use_remove_padding`: æ˜¯å¦ä½¿ç”¨ packed attentionï¼ˆFlash Attentionï¼‰
- `self.use_ulysses_sp`: æ˜¯å¦ä½¿ç”¨ Ulysses åºåˆ—å¹¶è¡Œ

---

## 5. BuPO ä¸¤é˜¶æ®µåˆ‡æ¢é€»è¾‘

### 5.1 compute_ref_log_prob() ä¸­çš„åˆ‡æ¢ï¼ˆç¬¬ 806-819 è¡Œï¼‰

```python
# æ–‡ä»¶: verl/workers/actor/dp_actor.py
# æ–¹æ³•: compute_ref_log_prob()
# ä½ç½®: ç¬¬ 806-819 è¡Œ

for micro_batch in micro_batches:
    micro_batch = micro_batch.to(get_device_id())
    model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}

    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        # ============ BuPO æ ¸å¿ƒé€»è¾‘ ============
        if self.config.internal_policy_interative:  # ç¬¬ 806 è¡Œ
            # åˆ¤æ–­å½“å‰æ˜¯ Phase 1 è¿˜æ˜¯ Phase 2
            if micro_batch.meta_info['global_steps'] <= self.config.iterative_steps:  # ç¬¬ 807 è¡Œ
                # Phase 1: ä½¿ç”¨å†…éƒ¨å±‚ç­–ç•¥
                entropy, log_probs = self._forward_micro_batch_layer_k(  # ç¬¬ 808 è¡Œ
                    model_inputs,
                    temperature=temperature,
                    calculate_entropy=calculate_entropy,
                    layer_k=self.config.internal_layer  # ä¾‹å¦‚ layer_k=5
                )
            else:
                # Phase 2: ä½¿ç”¨å®Œæ•´æ¨¡å‹ç­–ç•¥
                entropy, log_probs = self._forward_micro_batch(  # ç¬¬ 812 è¡Œ
                    model_inputs,
                    temperature=temperature,
                    calculate_entropy=calculate_entropy
                )
        # ============ æ ‡å‡† GRPO é€»è¾‘ ============
        else:
            entropy, log_probs = self._forward_micro_batch(
                model_inputs,
                temperature=temperature,
                calculate_entropy=calculate_entropy
            )
```

### 5.2 é€è¡Œè¯¦ç»†è§£æ

#### ç¬¬ 806 è¡Œ: `if self.config.internal_policy_interative:`

**é—®é¢˜**: ä»€ä¹ˆæ˜¯ `internal_policy_interative`ï¼Ÿ

**ç­”æ¡ˆ**:
- è¿™æ˜¯ä¸€ä¸ª**å¸ƒå°”é…ç½®å‚æ•°**ï¼Œæ§åˆ¶æ˜¯å¦å¯ç”¨ BuPO
- åœ¨è®­ç»ƒè„šæœ¬ä¸­è®¾ç½®:
  ```bash
  actor_rollout_ref.actor.internal_policy_interative=True
  ```
- **True**: å¯ç”¨ BuPOï¼ˆä¸¤é˜¶æ®µè®­ç»ƒï¼‰
- **False**: æ ‡å‡† GRPOï¼ˆå§‹ç»ˆä¼˜åŒ–å®Œæ•´æ¨¡å‹ï¼‰

#### ç¬¬ 807 è¡Œ: `if micro_batch.meta_info['global_steps'] <= self.config.iterative_steps:`

**é—®é¢˜**: `global_steps` å’Œ `iterative_steps` æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆ**:
- **`global_steps`**: å½“å‰è®­ç»ƒçš„å…¨å±€æ­¥æ•°ï¼ˆ1, 2, 3, ..., total_stepsï¼‰
- **`iterative_steps`**: Phase 1 çš„æŒç»­æ­¥æ•°ï¼ˆä¾‹å¦‚ 30ï¼‰
- **åˆ¤æ–­é€»è¾‘**:
  - `global_steps <= iterative_steps` (ä¾‹å¦‚ 1-30): Phase 1
  - `global_steps > iterative_steps` (ä¾‹å¦‚ 31-300): Phase 2

**ç¤ºä¾‹**:
```python
# å‡è®¾ iterative_steps = 30, total_steps = 300
# global_steps = 1:  Phase 1 (ä¼˜åŒ–å†…éƒ¨å±‚)
# global_steps = 30: Phase 1 (ä¼˜åŒ–å†…éƒ¨å±‚)
# global_steps = 31: Phase 2 (ä¼˜åŒ–å®Œæ•´æ¨¡å‹)
# global_steps = 300: Phase 2 (ä¼˜åŒ–å®Œæ•´æ¨¡å‹)
```

#### ç¬¬ 808-810 è¡Œ: `self._forward_micro_batch_layer_k(...)`

**é—®é¢˜**: è¿™ä¸ªå‡½æ•°åšä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆ**:
- **Phase 1 çš„æ ¸å¿ƒå‡½æ•°**: è®¡ç®—å†…éƒ¨å±‚ç­–ç•¥çš„ log_probs
- **è¾“å…¥**:
  - `model_inputs`: åŒ…å« input_ids, attention_mask ç­‰
  - `layer_k`: è¦ä¼˜åŒ–çš„å†…éƒ¨å±‚ç´¢å¼•ï¼ˆä¾‹å¦‚ 5ï¼‰
  - `temperature`: é‡‡æ ·æ¸©åº¦ï¼ˆä¾‹å¦‚ 1.0ï¼‰
- **è¾“å‡º**:
  - `log_probs`: å†…éƒ¨å±‚ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
  - `entropy`: ç­–ç•¥çš„ç†µï¼ˆå¯é€‰ï¼‰

**å…³é”®**: å®ƒä¼šè°ƒç”¨**è‡ªå®šä¹‰æ¨¡å‹**ï¼Œè·å–ç¬¬ k å±‚çš„éšè—çŠ¶æ€

#### ç¬¬ 812-814 è¡Œ: `self._forward_micro_batch(...)`

**é—®é¢˜**: è¿™å’Œä¸Šé¢æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**ç­”æ¡ˆ**:
- **Phase 2 å’Œæ ‡å‡† GRPO ä½¿ç”¨çš„å‡½æ•°**: è®¡ç®—å®Œæ•´æ¨¡å‹ç­–ç•¥çš„ log_probs
- **åŒºåˆ«**:
  | ç‰¹æ€§ | `_forward_micro_batch_layer_k` | `_forward_micro_batch` |
  |------|-------------------------------|------------------------|
  | ä½¿ç”¨é˜¶æ®µ | Phase 1 (BuPO) | Phase 2 + æ ‡å‡† GRPO |
  | è®¡ç®—å±‚ | ç¬¬ k å±‚ | æœ€åä¸€å±‚ |
  | è¾“å‡º | Ï€^k_Layer(a\|s) | Ï€Î¸(a\|s) |
  | æ¢¯åº¦æ›´æ–° | åªæ›´æ–° layers 0-k | æ›´æ–°æ‰€æœ‰å±‚ |

### 5.3 update_policy() ä¸­çš„åˆ‡æ¢ï¼ˆç¬¬ 906-918 è¡Œï¼‰

**è¿™æ˜¯ç¬¬äºŒå¤„ç›¸åŒçš„åˆ‡æ¢é€»è¾‘**ï¼Œåœ¨ç­–ç•¥æ›´æ–°æ—¶ä½¿ç”¨ï¼š

```python
# æ–‡ä»¶: verl/workers/actor/dp_actor.py
# æ–¹æ³•: update_policy()
# ä½ç½®: ç¬¬ 906-918 è¡Œ

calculate_entropy = False
if entropy_coeff != 0:
    calculate_entropy = True

# ============ BuPO æ ¸å¿ƒé€»è¾‘ï¼ˆä¸ä¸Šé¢å®Œå…¨ç›¸åŒï¼‰============
if self.config.internal_policy_interative:  # ç¬¬ 906 è¡Œ
    if micro_batch.meta_info['global_steps'] <= self.config.iterative_steps:  # ç¬¬ 907 è¡Œ
        # Phase 1
        entropy, log_prob = self._forward_micro_batch_layer_k(  # ç¬¬ 908 è¡Œ
           model_inputs,
           temperature=temperature,
           calculate_entropy=calculate_entropy,
           layer_k=self.config.internal_layer
        )
    else:
        # Phase 2
        entropy, log_prob = self._forward_micro_batch(  # ç¬¬ 912 è¡Œ
            model_inputs,
            temperature=temperature,
            calculate_entropy=calculate_entropy
        )
else:
    # æ ‡å‡† GRPO
    entropy, log_prob = self._forward_micro_batch(
        model_inputs,
        temperature=temperature,
        calculate_entropy=calculate_entropy
    )
```

**ä¸ºä»€ä¹ˆæœ‰ä¸¤å¤„ï¼Ÿ**

1. **ç¬¬ä¸€å¤„**ï¼ˆ`compute_ref_log_prob`ï¼‰: ç”¨äºè®¡ç®—å‚è€ƒç­–ç•¥çš„ log_probsï¼ˆæ¨ç†æ¨¡å¼ï¼Œä¸è®¡ç®—æ¢¯åº¦ï¼‰
2. **ç¬¬äºŒå¤„**ï¼ˆ`update_policy`ï¼‰: ç”¨äºç­–ç•¥æ›´æ–°ï¼ˆè®­ç»ƒæ¨¡å¼ï¼Œè®¡ç®—æ¢¯åº¦ï¼‰

---

## 6. å†…éƒ¨å±‚å‰å‘ä¼ æ’­

### 6.1 _forward_micro_batch_layer_k() æ–¹æ³•

```
æ–‡ä»¶: verl/workers/actor/dp_actor.py
æ–¹æ³•: _forward_micro_batch_layer_k()
ä½ç½®: ç¬¬ 357-555 è¡Œï¼ˆçº¦ 200 è¡Œï¼‰
```

è¿™æ˜¯ **BuPO æœ€æ ¸å¿ƒçš„å‡½æ•°**ï¼Œå®ç°äº†å†…éƒ¨å±‚ç­–ç•¥çš„å‰å‘ä¼ æ’­ã€‚

### 6.2 å‡½æ•°ç­¾åï¼ˆç¬¬ 357-359 è¡Œï¼‰

```python
def _forward_micro_batch_layer_k(
    self,
    micro_batch,
    temperature,
    calculate_entropy=False,
    layer_k=None
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Returns:
        entropy: # (bs, response_len)
        log_probs: # (bs, response_len)
    """
```

**å‚æ•°è¯´æ˜**:
- `micro_batch`: ä¸€ä¸ªå°æ‰¹æ¬¡çš„æ•°æ®
- `temperature`: é‡‡æ ·æ¸©åº¦ï¼ˆæ§åˆ¶åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦ï¼‰
- `calculate_entropy`: æ˜¯å¦è®¡ç®—ç†µ
- `layer_k`: å†…éƒ¨å±‚ç´¢å¼•ï¼ˆä¾‹å¦‚ 5ï¼‰

**è¿”å›å€¼**:
- `entropy`: ç­–ç•¥ç†µï¼Œshape=(batch_size, response_length)
- `log_probs`: å¯¹æ•°æ¦‚ç‡ï¼Œshape=(batch_size, response_length)

### 6.3 è·å–è¾“å…¥æ•°æ®ï¼ˆç¬¬ 366-387 è¡Œï¼‰

```python
response_length = micro_batch["responses"].size(-1)

# å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒç­‰ï¼‰
multi_modal_inputs = {}
if "multi_modal_inputs" in micro_batch.keys():
    # ... å¤„ç†å›¾åƒè¾“å…¥ ...

with torch.autocast(device_type=self.device_name, dtype=torch.bfloat16):
    input_ids = micro_batch["input_ids"]  # (batch_size, total_length)
    batch_size, seqlen = input_ids.shape
    attention_mask = micro_batch["attention_mask"]  # (batch_size, total_length)
    position_ids = micro_batch["position_ids"]  # (batch_size, total_length)
    entropy = None
```

**é€è¡Œè§£æ**:

**ç¬¬ 366 è¡Œ**: `response_length = micro_batch["responses"].size(-1)`
- **responses**: æ¨¡å‹ç”Ÿæˆçš„å“åº”éƒ¨åˆ†
- **ä½œç”¨**: åç»­åªè®¡ç®—å“åº”éƒ¨åˆ†çš„ log_probsï¼ˆä¸åŒ…æ‹¬ promptï¼‰

**ç¬¬ 379 è¡Œ**: `with torch.autocast(..., dtype=torch.bfloat16):`
- **æ··åˆç²¾åº¦è®­ç»ƒ**: ä½¿ç”¨ bfloat16 åŠ é€Ÿè®¡ç®—
- **è‡ªåŠ¨ç±»å‹è½¬æ¢**: PyTorch è‡ªåŠ¨åœ¨ bfloat16 å’Œ float32 ä¹‹é—´åˆ‡æ¢

**ç¬¬ 380-383 è¡Œ**: æå–è¾“å…¥
- `input_ids`: Token IDsï¼Œå½¢çŠ¶ (batch_size, total_length)
  - `total_length = prompt_length + response_length`
- `attention_mask`: æ³¨æ„åŠ›æ©ç ï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆ tokenï¼Œ0 è¡¨ç¤ºå¡«å……
- `position_ids`: ä½ç½®ç¼–ç 

### 6.4 Packed Attention å¤„ç†ï¼ˆç¬¬ 388-438 è¡Œï¼‰

**ä»€ä¹ˆæ˜¯ Packed Attentionï¼Ÿ**

æ ‡å‡† Attention:
```
Batch 1: [token1, token2, token3, PAD, PAD]
Batch 2: [token1, PAD, PAD, PAD, PAD]
Batch 3: [token1, token2, token3, token4, PAD]
```

Packed Attention (Flash Attention):
```
Packed: [batch1_token1, batch1_token2, batch1_token3, batch2_token1, batch3_token1, ...]
         ^                                              ^             ^
         cu_seqlens[0]                                  cu_seqlens[1] cu_seqlens[2]
```

**ä¼˜ç‚¹**:
- å»é™¤ PAD tokenï¼Œå‡å°‘è®¡ç®—é‡
- æ”¯æŒå˜é•¿åºåˆ—ï¼Œæé«˜æ•ˆç‡

```python
if self.use_remove_padding:  # ç¬¬ 388 è¡Œ
    # å»é™¤ paddingï¼Œå˜æˆ packed æ ¼å¼
    input_ids_rmpad, indices, cu_seqlens, *_ = unpad_input(
        input_ids.unsqueeze(-1), attention_mask
    )  # input_ids_rmpad shape: (total_nnz, 1)
    input_ids_rmpad = input_ids_rmpad.transpose(0, 1)  # (1, total_nnz)

    # åŒæ ·å¤„ç† position_ids
    position_ids_rmpad = index_first_axis(
        rearrange(position_ids.unsqueeze(-1), "b s ... -> (b s) ..."),
        indices
    ).transpose(0, 1)
```

**é€è¡Œè§£æ**:

**ç¬¬ 388 è¡Œ**: `if self.use_remove_padding:`
- **åˆ¤æ–­**: æ˜¯å¦ä½¿ç”¨ packed attentionï¼ˆæ¨èï¼Œæ›´å¿«ï¼‰

**ç¬¬ 389-391 è¡Œ**: `unpad_input(...)`
- **Flash Attention å·¥å…·**: å»é™¤ padding tokens
- **è¾“å…¥**: `input_ids` + `attention_mask`
- **è¾“å‡º**:
  - `input_ids_rmpad`: å‹ç¼©åçš„ token IDs
  - `indices`: æœ‰æ•ˆ token çš„ç´¢å¼•
  - `cu_seqlens`: ç´¯ç§¯åºåˆ—é•¿åº¦ï¼ˆç”¨äºåˆ†éš”ä¸åŒæ ·æœ¬ï¼‰
    ```python
    # ç¤ºä¾‹
    cu_seqlens = [0, 3, 4, 8]  # è¡¨ç¤º:
    # Batch 0: tokens 0-2 (3 ä¸ª tokens)
    # Batch 1: tokens 3-3 (1 ä¸ª token)
    # Batch 2: tokens 4-7 (4 ä¸ª tokens)
    ```

**ç¬¬ 414 è¡Œ**: `input_ids_rmpad_rolled = torch.roll(input_ids_rmpad, shifts=-1, dims=1)`
- **ä½œç”¨**: å°†åºåˆ—å‘å·¦ç§»åŠ¨ä¸€ä½ï¼Œç”¨äºè®¡ç®— log_probs
- **ä¸ºä»€ä¹ˆï¼Ÿ** å› ä¸ºè¦è®¡ç®— P(token_t | context<t)
  ```python
  åŸå§‹: [A, B, C, D]
  rolled: [B, C, D, A]  # ç”¨äºåŒ¹é… labels
  ```

### 6.5 è°ƒç”¨æ¨¡å‹ï¼ˆç¬¬ 446-454 è¡Œï¼‰

```python
output = self.actor_module(
    input_ids=input_ids_rmpad,  # (1, total_nnz)
    attention_mask=None,  # packed attention ä¸éœ€è¦
    position_ids=position_ids_rmpad,
    **multi_modal_inputs,
    use_cache=False,  # è®­ç»ƒæ—¶ä¸ç¼“å­˜ KV
    output_hidden_states=True,  # â˜… å…³é”®: è¾“å‡ºæ‰€æœ‰å±‚çš„éšè—çŠ¶æ€
    **extra_args,
)
```

**é€è¡Œè§£æ**:

**ç¬¬ 446-454 è¡Œ**: æ¨¡å‹å‰å‘ä¼ æ’­
- **input_ids**: å‹ç¼©åçš„è¾“å…¥
- **attention_mask=None**: packed attention é€šè¿‡ `cu_seqlens` å¤„ç†
- **output_hidden_states=True**: â­ **BuPO çš„å…³é”®**
  - è®©æ¨¡å‹è¿”å›**æ‰€æœ‰å±‚**çš„éšè—çŠ¶æ€
  - æ ‡å‡†æ¨¡å‹åªè¿”å›æœ€åä¸€å±‚
  - è‡ªå®šä¹‰æ¨¡å‹ä¼šè¿”å› `hidden_states[0], hidden_states[1], ..., hidden_states[L]`

**è¿”å›çš„ output å¯¹è±¡åŒ…å«**:
- `output.logits`: æœ€åä¸€å±‚çš„ logits (vocabulary distribution)
- `output.hidden_states`: æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ï¼ˆtupleï¼‰
- `output.mid_layer_logits`: â­ **BuPO æ·»åŠ çš„å­—æ®µ**
  - è‡ªå®šä¹‰æ¨¡å‹è®¡ç®—çš„å†…éƒ¨å±‚ logits
  - `mid_layer_logits[k]` = ç¬¬ k å±‚çš„ logits

### 6.6 æå–å†…éƒ¨å±‚ logitsï¼ˆç¬¬ 458-462 è¡Œï¼‰

```python
if self.use_fused_kernels:  # ç¬¬ 455 è¡Œ
    # ä½¿ç”¨èåˆå†…æ ¸ï¼ˆä¸å¸¸ç”¨ï¼‰
    log_probs = output.log_probs.squeeze(0)
    entropy_rmpad = output.entropy.squeeze(0)
else:  # ç¬¬ 458 è¡Œ - å¸¸ç”¨è·¯å¾„
    # â˜…â˜…â˜… å…³é”®: ä»è‡ªå®šä¹‰æ¨¡å‹è·å–ç¬¬ k å±‚çš„ logits â˜…â˜…â˜…
    logits_rmpad = output.mid_layer_logits[layer_k].squeeze(0)  # ç¬¬ 460 è¡Œ
    logits_rmpad.div_(temperature)  # ç¬¬ 461 è¡Œ
```

**é€è¡Œè§£æ**:

**ç¬¬ 460 è¡Œ**: `logits_rmpad = output.mid_layer_logits[layer_k].squeeze(0)`
- **mid_layer_logits**: è‡ªå®šä¹‰æ¨¡å‹è¿”å›çš„å­—å…¸
  - `mid_layer_logits[5]`: ç¬¬ 5 å±‚çš„ logits
  - Shape: (1, total_nnz, vocab_size)
- **squeeze(0)**: å»æ‰ç¬¬ä¸€ä¸ªç»´åº¦
  - Shape å˜ä¸º: (total_nnz, vocab_size)

**é—®é¢˜**: `mid_layer_logits` æ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Ÿ

**ç­”æ¡ˆ**: åœ¨è‡ªå®šä¹‰æ¨¡å‹ä¸­ï¼ˆç¨åè¯¦è§£ï¼‰ï¼š
```python
# verl/models/custom_model/modeling_qwen3.py: è¡Œ 593-596
startk = int(self.config.internal_layer)  # ä¾‹å¦‚ 5
for i in range(startk, startk+1):
    # H^k E^T_u (è®ºæ–‡å…¬å¼ 6)
    internal_logits[i] = self.lm_head(outputs.hidden_states[i+1])
```

**ç¬¬ 461 è¡Œ**: `logits_rmpad.div_(temperature)`
- **æ¸©åº¦ç¼©æ”¾**: logits / temperature
- **ä½œç”¨**: æ§åˆ¶åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦
  - temperature = 1.0: ä¸å˜
  - temperature > 1.0: æ›´å¹³æ»‘ï¼ˆç†µå¢åŠ ï¼‰
  - temperature < 1.0: æ›´å°–é”ï¼ˆç†µå‡å°‘ï¼‰

### 6.7 è®¡ç®— Log Probsï¼ˆç¬¬ 466-470 è¡Œï¼‰

```python
log_probs = logprobs_from_logits(
    logits=logits_rmpad,  # (total_nnz, vocab_size)
    labels=input_ids_rmpad_rolled,  # (total_nnz,) - rolled labels
    inplace_backward=inplace_backward,  # æ˜¯å¦åŸåœ°æ¢¯åº¦
)
```

**é€è¡Œè§£æ**:

**logprobs_from_logits() å‡½æ•°**:
```python
# å®ç°é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
def logprobs_from_logits(logits, labels, inplace_backward):
    # 1. è®¡ç®— log softmax
    log_probs_all = F.log_softmax(logits, dim=-1)  # (total_nnz, vocab_size)

    # 2. æå– labels å¯¹åº”çš„ log_probs
    log_probs = torch.gather(log_probs_all, dim=-1, index=labels.unsqueeze(-1))

    # 3. å»æ‰æœ€åä¸€ç»´
    log_probs = log_probs.squeeze(-1)  # (total_nnz,)

    return log_probs
```

**ç¤ºä¾‹**:
```python
# å‡è®¾ vocab_size = 3, total_nnz = 4
logits = [[2.0, 1.0, 0.5],  # token 0 çš„ logits
          [1.5, 2.5, 1.0],  # token 1 çš„ logits
          [0.5, 1.0, 2.0],  # token 2 çš„ logits
          [2.0, 2.0, 2.0]]  # token 3 çš„ logits

labels = [1, 2, 2, 0]  # å®é™…ç”Ÿæˆçš„ token IDs

# log_softmax å
log_probs_all = [[-0.5, -1.5, -2.0],
                 [-1.5, -0.5, -2.0],
                 [-2.5, -2.0, -1.0],
                 [-1.1, -1.1, -1.1]]

# gather æå–å¯¹åº” labels çš„ log_probs
log_probs = [-1.5,  # log P(token_1=1 | context)
             -2.0,  # log P(token_2=2 | context)
             -1.0,  # log P(token_3=2 | context)
             -1.1]  # log P(token_4=0 | context)
```

### 6.8 Pad å›åŸå§‹ shapeï¼ˆç¬¬ 505-522 è¡Œï¼‰

```python
if calculate_entropy:
    full_entropy = pad_input(
        hidden_states=entropy_rmpad.unsqueeze(-1),
        indices=indices,  # æ¥è‡ª unpad_input
        batch=batch_size,
        seqlen=seqlen,
    )

full_log_probs = pad_input(
    hidden_states=log_probs.unsqueeze(-1),
    indices=indices,
    batch=batch_size,
    seqlen=seqlen,
)

# åªè¿”å› response éƒ¨åˆ†
if calculate_entropy:
    entropy = full_entropy.squeeze(-1)[:, -response_length - 1 : -1]
log_probs = full_log_probs.squeeze(-1)[:, -response_length - 1 : -1]
```

**é€è¡Œè§£æ**:

**pad_input() å‡½æ•°**:
- **ä½œç”¨**: å°† packed æ ¼å¼è¿˜åŸå› (batch_size, seqlen) æ ¼å¼
- **åŸç†**: ä½¿ç”¨ `indices` å°† tokens æ”¾å›åŸä½ç½®ï¼Œå…¶ä»–ä½ç½®å¡«å…… 0

**ç¬¬ 521-522 è¡Œ**: `[:, -response_length - 1 : -1]`
- **ä½œç”¨**: åªä¿ç•™å“åº”éƒ¨åˆ†çš„ log_probs
- **ä¸ºä»€ä¹ˆ -1ï¼Ÿ** å› ä¸ºæœ€åä¸€ä¸ª token æ²¡æœ‰ä¸‹ä¸€ä¸ª token å¯é¢„æµ‹
- **ç¤ºä¾‹**:
  ```python
  total_length = 10 (prompt=6, response=4)
  response_length = 4

  input_ids:    [p1, p2, p3, p4, p5, p6, r1, r2, r3, r4]
  log_probs:    [l1, l2, l3, l4, l5, l6, l7, l8, l9, l10]
                                        ^           ^
                                        æå– l7-l9

  [:, -5:-1] = [:, -response_length-1:-1] = [l7, l8, l9]
  ```

### 6.9 è¿”å›ç»“æœ

```python
return log_probs, entropys
```

**è¿”å›å€¼**:
- `log_probs`: shape=(batch_size, response_length)
- `entropys`: shape=(batch_size, response_length) æˆ– None

---

## 7. è‡ªå®šä¹‰æ¨¡å‹å®ç°

### 7.1 ä¸ºä»€ä¹ˆéœ€è¦è‡ªå®šä¹‰æ¨¡å‹ï¼Ÿ

æ ‡å‡† Transformers æ¨¡å‹ï¼š
```python
output = model(input_ids, attention_mask)
# output.logits: åªæœ‰æœ€åä¸€å±‚çš„è¾“å‡º
# output.hidden_states: éœ€è¦æ‰‹åŠ¨è®¾ç½® output_hidden_states=True
```

**é—®é¢˜**:
1. æ— æ³•ç›´æ¥è·å–ä¸­é—´å±‚çš„ logits
2. éœ€è¦æ‰‹åŠ¨è®¡ç®— `H^k E^T_u`

BuPO è‡ªå®šä¹‰æ¨¡å‹ï¼š
```python
output = custom_model(input_ids, attention_mask, output_hidden_states=True)
# output.logits: æœ€åä¸€å±‚
# output.hidden_states: æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ (è‡ªåŠ¨)
# output.mid_layer_logits: å†…éƒ¨å±‚çš„ logits (è‡ªåŠ¨è®¡ç®—)
```

### 7.2 è‡ªå®šä¹‰æ¨¡å‹ä½ç½®

```
verl/models/custom_model/
â”œâ”€â”€ modeling_qwen2.py
â”œâ”€â”€ modeling_qwen3.py      # â† æˆ‘ä»¬é‡ç‚¹çœ‹è¿™ä¸ª
â”œâ”€â”€ modeling_llama.py
â”œâ”€â”€ configuration_qwen2.py
â”œâ”€â”€ configuration_qwen3.py
â””â”€â”€ configuration_llama.py
```

### 7.3 Qwen3ForCausalLM.forward() æ–¹æ³•

```
æ–‡ä»¶: verl/models/custom_model/modeling_qwen3.py
æ–¹æ³•: Qwen3ForCausalLM.forward()
ä½ç½®: ç¬¬ 523-597 è¡Œ
```

#### æ ¸å¿ƒå®ç°ï¼ˆç¬¬ 585-597 è¡Œï¼‰

```python
# ç¬¬ 523-584 è¡Œ: æ ‡å‡†çš„ forward é€»è¾‘ï¼ˆä¸ HuggingFace ç›¸åŒï¼‰
outputs = self.model(
    input_ids=input_ids,
    attention_mask=attention_mask,
    position_ids=position_ids,
    ...
    output_hidden_states=output_hidden_states,  # è¾“å‡ºæ‰€æœ‰å±‚
    ...
)

hidden_states = outputs[0]  # æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
logits = self.lm_head(hidden_states)  # æœ€åä¸€å±‚çš„ logits

# ç¬¬ 577-584 è¡Œ: åˆ›å»ºæ ‡å‡†è¾“å‡ºå¯¹è±¡
output = CausalLMOutputWithPastNew(
    loss=loss,
    logits=logits,
    past_key_values=outputs.past_key_values,
    hidden_states=outputs.hidden_states,  # æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€
    attentions=outputs.attentions,
)

# ============ BuPO æ·»åŠ çš„æ ¸å¿ƒä»£ç  ============
internal_logits = {}  # ç¬¬ 585 è¡Œ

"""
Extraction of Internal Hidden States

args:
    startk: which layer used as internal layer policy.
            For Qwen3-4B, startk âˆˆ [0, 35].
            Here, startk = 0 equals to layer 1 in paper.
"""
startk = int(self.config.internal_layer)  # ç¬¬ 593 è¡Œ
# ä¾‹å¦‚: startk = 5

for i in range(startk, startk+1):  # ç¬¬ 594 è¡Œ
    # åªå¾ªç¯ä¸€æ¬¡ï¼Œè®¡ç®—ç¬¬ startk å±‚çš„ logits
    internal_logits[i] = self.lm_head(outputs.hidden_states[i+1][:, slice_indices, :])  # ç¬¬ 595 è¡Œ
    # â†‘ è¿™å°±æ˜¯è®ºæ–‡ä¸­çš„ H^k E^T_u (å…¬å¼ 6)

output.mid_layer_logits = internal_logits  # ç¬¬ 596 è¡Œ
return output  # ç¬¬ 597 è¡Œ
```

**é€è¡Œè¯¦ç»†è§£æ**:

**ç¬¬ 585 è¡Œ**: `internal_logits = {}`
- åˆ›å»ºç©ºå­—å…¸ï¼Œç”¨äºå­˜å‚¨å†…éƒ¨å±‚çš„ logits

**ç¬¬ 593 è¡Œ**: `startk = int(self.config.internal_layer)`
- **ä»é…ç½®è¯»å–**: è¦è®¡ç®—å“ªä¸€å±‚çš„å†…éƒ¨ logits
- **é…ç½®è·¯å¾„**: `actor_rollout_ref.model.override_config.internal_layer=5`
- **ç¤ºä¾‹**: startk = 5 è¡¨ç¤ºç¬¬ 5 å±‚ï¼ˆä»£ç ä» 0 å¼€å§‹ï¼‰

**ç¬¬ 594 è¡Œ**: `for i in range(startk, startk+1):`
- **range(5, 6)**: åªå¾ªç¯ä¸€æ¬¡ï¼Œi=5
- **ä¸ºä»€ä¹ˆä¸ç›´æ¥å†™ i=startkï¼Ÿ**
  - ä»£ç è®¾è®¡ä¸ºå¯ä»¥è®¡ç®—å¤šå±‚ï¼Œä½†å½“å‰åªç”¨ä¸€å±‚

**ç¬¬ 595 è¡Œ**: `internal_logits[i] = self.lm_head(outputs.hidden_states[i+1][:, slice_indices, :])`
- **è¿™æ˜¯ BuPO æœ€æ ¸å¿ƒçš„ä¸€è¡Œä»£ç ï¼**
- è®©æˆ‘ä»¬åˆ†è§£ï¼š

**1. `outputs.hidden_states`**:
- **ç±»å‹**: Tuple of Tensors
- **é•¿åº¦**: L + 1ï¼ˆL æ˜¯ Transformer å±‚æ•°ï¼‰
- **å†…å®¹**:
  ```python
  hidden_states[0]:  Embedding å±‚è¾“å‡º
  hidden_states[1]:  Layer 0 çš„è¾“å‡º
  hidden_states[2]:  Layer 1 çš„è¾“å‡º
  ...
  hidden_states[k+1]: Layer k çš„è¾“å‡º  # â† æˆ‘ä»¬è¦è¿™ä¸ª
  ...
  hidden_states[L]:  Layer L-1 çš„è¾“å‡ºï¼ˆæœ€åä¸€å±‚ï¼‰
  ```

**2. `outputs.hidden_states[i+1]`**:
- **i=5**: `hidden_states[6]` = Layer 5 çš„è¾“å‡º
- **ä¸ºä»€ä¹ˆ +1ï¼Ÿ** å› ä¸º `hidden_states[0]` æ˜¯ embedding

**3. `[:, slice_indices, :]`**:
- **ä½œç”¨**: åªæå–éœ€è¦è®¡ç®— logits çš„éƒ¨åˆ†
- **slice_indices**: é€šå¸¸æ˜¯å“åº”éƒ¨åˆ†çš„ç´¢å¼•

**4. `self.lm_head(...)`**:
- **lm_head**: Language Model Headï¼ˆè¾“å‡ºæŠ•å½±å±‚ï¼‰
- **ä½œç”¨**: å°†éšè—çŠ¶æ€æŠ•å½±åˆ°è¯è¡¨ç©ºé—´
- **æ•°å­¦**: `logits = H^k W^T` ï¼ˆå…¶ä¸­ W æ˜¯ lm_head çš„æƒé‡ï¼Œä¹Ÿå°±æ˜¯ E_uï¼‰
- **è¾“å…¥ shape**: (batch_size, seq_len, hidden_dim)
- **è¾“å‡º shape**: (batch_size, seq_len, vocab_size)

**å®Œæ•´æµç¨‹å›¾**:
```
Layer 5 çš„è¾“å‡º (H^5)
    â†“ shape: (batch, seq, hidden_dim)
self.lm_head (E_u^T)
    â†“ çŸ©é˜µä¹˜æ³•: H^5 @ E_u^T
logits^5
    â†“ shape: (batch, seq, vocab_size)
softmax(logits^5) â†’ Ï€^5_Layer (å†…éƒ¨å±‚ç­–ç•¥)
```

**è¿™å¯¹åº”è®ºæ–‡çš„å…¬å¼ 6**:
```
Ï€^l_Layer â‰¡ P^l_Layer = softmax(H^l E^T_u)
```

**ç¬¬ 596 è¡Œ**: `output.mid_layer_logits = internal_logits`
- å°†è®¡ç®—çš„å†…éƒ¨ logits æ·»åŠ åˆ°è¾“å‡ºå¯¹è±¡
- **mid_layer_logits[5]**: ç¬¬ 5 å±‚çš„ logits

### 7.4 CausalLMOutputWithPastNew ç±»

```
æ–‡ä»¶: verl/models/custom_model/modeling_qwen3.py
ä½ç½®: ç¬¬ 88-120 è¡Œ
```

```python
class CausalLMOutputWithPastNew(ModelOutput):
    """
    æ‰©å±•çš„è¾“å‡ºç±»ï¼Œæ·»åŠ äº† mid_layer_logits å­—æ®µ
    """
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    past_key_values: Optional[UserDict[str, Cache]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    mid_layer_logits: Optional[UserDict[int, torch.FloatTensor]] = None  # â† æ–°å¢
```

**mid_layer_logits**:
- **ç±»å‹**: å­—å…¸ `{layer_index: logits_tensor}`
- **ç¤ºä¾‹**:
  ```python
  mid_layer_logits = {
      5: tensor of shape (batch, seq, vocab_size)
  }
  ```

### 7.5 æ¨¡å‹æ›¿æ¢æœºåˆ¶

```
æ–‡ä»¶: verl/workers/fsdp_workers.py
ä½ç½®: ç¬¬ 249-256 è¡Œ
```

```python
if hasattr(self.config.actor, "internal_policy_interative") and \
   self.config.actor.internal_policy_interative:
    # å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹
    from verl.models.custom_model import modeling_qwen2 as custom_modeling_qwen2
    from verl.models.custom_model import modeling_qwen3 as custom_modeling_qwen3
    from verl.models.custom_model import modeling_llama as custom_modeling_llama

    # æ›¿æ¢ sys.modules ä¸­çš„æ¨¡å‹
    sys.modules["transformers.models.qwen2.modeling_qwen2"] = custom_modeling_qwen2
    sys.modules["transformers.models.qwen3.modeling_qwen3"] = custom_modeling_qwen3
    sys.modules["transformers.models.llama.modeling_llama"] = custom_modeling_llama
```

**é€è¡Œè§£æ**:

**ç¬¬ 249 è¡Œ**: æ£€æŸ¥æ˜¯å¦å¯ç”¨ BuPO

**ç¬¬ 250-252 è¡Œ**: å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹æ¨¡å—

**ç¬¬ 253-255 è¡Œ**: **å…³é”®çš„æ¨¡å—æ›¿æ¢**
- **sys.modules**: Python çš„æ¨¡å—ç¼“å­˜
- **ä½œç”¨**: å°† HuggingFace çš„æ ‡å‡†æ¨¡å‹æ›¿æ¢ä¸ºè‡ªå®šä¹‰æ¨¡å‹
- **åŸç†**: å½“åç»­ä»£ç  `import transformers.models.qwen3.modeling_qwen3` æ—¶ï¼Œå®é™…å¯¼å…¥çš„æ˜¯è‡ªå®šä¹‰ç‰ˆæœ¬

**ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ**
- HuggingFace çš„ AutoModel ä¼šè‡ªåŠ¨å¯¼å…¥æ¨¡å‹
- é€šè¿‡æ›¿æ¢ sys.modulesï¼Œæ— éœ€ä¿®æ”¹ HuggingFace çš„ä»£ç 
- ä¿æŒå…¼å®¹æ€§ï¼Œåˆ‡æ¢æ–¹ä¾¿

---

## 8. Loss è®¡ç®—ä¸åå‘ä¼ æ’­

### 8.1 PPO Loss è®¡ç®—ï¼ˆç¬¬ 925-933 è¡Œï¼‰

```python
# è·å– policy loss è®¡ç®—å‡½æ•°
loss_mode = self.config.policy_loss.get("loss_mode", "vanilla")
policy_loss_fn = get_policy_loss_fn(loss_mode)

# è®¡ç®— policy loss
pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = policy_loss_fn(
    old_log_prob=old_log_prob,  # æ¥è‡ª rollout çš„ old policy log prob
    log_prob=log_prob,  # å½“å‰ policy çš„ log prob
    advantages=advantages,  # ä¼˜åŠ¿å‡½æ•° (GRPO è®¡ç®—)
    response_mask=response_mask,  # å“åº”éƒ¨åˆ†çš„ mask
    loss_agg_mode=loss_agg_mode,  # "token-mean" æˆ– "sample-mean"
    config=self.config,
    rollout_log_probs=rollout_log_probs,
)
```

**é€è¡Œè§£æ**:

**policy_loss_fn() å‡½æ•°** (vanilla PPO loss):
```python
def compute_policy_loss_vanilla(old_log_prob, log_prob, advantages, response_mask, ...):
    """
    è®ºæ–‡å…¬å¼ 4 (GRPO) çš„å®ç°
    """
    # 1. è®¡ç®— importance ratio
    ratio = torch.exp(log_prob - old_log_prob)  # r_i,t = Ï€_Î¸ / Ï€_Î¸_old

    # 2. è®¡ç®— clipped objective
    clipped_ratio = torch.clamp(ratio, 1 - Îµ, 1 + Îµ)  # clip(r, 1-Îµ, 1+Îµ)

    # 3. è®¡ç®— surrogate loss
    loss1 = ratio * advantages
    loss2 = clipped_ratio * advantages
    pg_loss = -torch.min(loss1, loss2)  # min{r*A, clip(r)*A}

    # 4. èšåˆ loss
    pg_loss = agg_loss(pg_loss, response_mask, loss_agg_mode)

    return pg_loss, ...
```

**å…³é”®ç‚¹**:

**Phase 1 (BuPO) çš„ importance ratio**:
```python
# log_prob æ¥è‡ª _forward_micro_batch_layer_k()
rÌ‚_i,t = exp(log_prob_k - old_log_prob_k)
      = Ï€^k_Layer(o_t | context) / Ï€^k_Layer,old(o_t | context)
```

**Phase 2 çš„ importance ratio**:
```python
# log_prob æ¥è‡ª _forward_micro_batch()
r_i,t = exp(log_prob - old_log_prob)
      = Ï€_Î¸(o_t | context) / Ï€_Î¸_old(o_t | context)
```

**è¿™å®ç°äº†è®ºæ–‡å…¬å¼ 10ï¼ˆInterGRPOï¼‰çš„ importance ratio åˆ‡æ¢ï¼**

### 8.2 æ·»åŠ  Entropy Regularizationï¼ˆç¬¬ 935-940 è¡Œï¼‰

```python
if entropy_coeff != 0:
    entropy_loss = agg_loss(
        loss_mat=entropy,
        loss_mask=response_mask,
        loss_agg_mode=loss_agg_mode
    )
    # æ€» loss = policy loss - entropy_coeff * entropy_loss
    policy_loss = pg_loss - entropy_loss * entropy_coeff
else:
    policy_loss = pg_loss
```

**ä½œç”¨**:
- **Entropy Regularization**: é¼“åŠ±ç­–ç•¥ä¿æŒæ¢ç´¢æ€§
- **entropy_coeff**: ç†µçš„æƒé‡ï¼ˆé€šå¸¸è®¾ä¸º 0ï¼ŒBuPO ä¸ä½¿ç”¨ï¼‰

### 8.3 åå‘ä¼ æ’­ï¼ˆç¬¬ 951-965 è¡Œï¼‰

```python
# ç¼©æ”¾ lossï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
loss = policy_loss * loss_scale_factor
loss.backward()  # åå‘ä¼ æ’­

# æ¢¯åº¦è£å‰ª
if self.config.grad_clip > 0:
    if self.config.strategy == "fsdp2":
        fsdp2_clip_grad_norm_(
            self.actor_module,
            max_norm=self.config.grad_clip
        )
    else:
        torch.nn.utils.clip_grad_norm_(
            self.actor_module.parameters(),
            self.config.grad_clip
        )

# æ›´æ–°å‚æ•°
self.actor_optimizer.step()
self.actor_scheduler.step()
```

**é€è¡Œè§£æ**:

**loss.backward()**:
- **åå‘ä¼ æ’­**: è®¡ç®—æ¢¯åº¦
- **å…³é”®**:
  - **Phase 1**: åªæœ‰ layers 0-k çš„å‚æ•°ä¼šæ”¶åˆ°æ¢¯åº¦
  - **Phase 2**: æ‰€æœ‰å±‚çš„å‚æ•°éƒ½ä¼šæ”¶åˆ°æ¢¯åº¦

**ä¸ºä»€ä¹ˆ Phase 1 åªæ›´æ–° 0-k å±‚ï¼Ÿ**

**åŸå› **: æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰çš„æ¢¯åº¦æµ

```python
# ç¬¬ k å±‚çš„è¾“å‡º
H^k = H^0 + A^1 + F^1 + ... + A^k + F^k

# å¯¹ç¬¬ k+1 å±‚çš„å‚æ•° Î¸_{k+1} æ±‚å¯¼
âˆ‚H^k / âˆ‚Î¸_{k+1} = 0  # å› ä¸º H^k ä¸ä¾èµ– Î¸_{k+1}

# å› æ­¤ç¬¬ k+1 å±‚åŠä»¥ä¸Šçš„å‚æ•°æ¢¯åº¦ä¸º 0ï¼Œä¸ä¼šè¢«æ›´æ–°
```

**è¿™å®ç°äº†è®ºæ–‡å…¬å¼ 16 çš„æ¢¯åº¦æµæ§åˆ¶ï¼**

**æ¢¯åº¦è£å‰ª**:
- **ä½œç”¨**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **max_norm**: æ¢¯åº¦çš„æœ€å¤§èŒƒæ•°ï¼ˆé»˜è®¤ 1.0ï¼‰

**optimizer.step()**:
- **AdamW ä¼˜åŒ–å™¨**: æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°

---

## 9. å®Œæ•´è®­ç»ƒå¾ªç¯

### 9.1 å•ä¸ªè®­ç»ƒ Step çš„æµç¨‹

```
RayPPOTrainer.train_step(step)
    â†“
1. Rollout Phase (ç”Ÿæˆå“åº”)
    â””â”€ rollout_module.generate()  # vLLM ç”Ÿæˆ
    â””â”€ è¿”å›: responses, rollout_log_probs
    â†“
2. Reward Computation
    â””â”€ reward_manager.compute_rewards(responses)
    â””â”€ è¿”å›: rewards
    â†“
3. Compute Advantages (GRPO)
    â””â”€ advantages = (rewards - mean(rewards)) / std(rewards)
    â†“
4. Actor Update (BuPO æ ¸å¿ƒ)
    â””â”€ actor.update_policy(data)
        â”œâ”€ åˆ¤æ–­ global_steps <= iterative_steps?
        â”œâ”€ Phase 1: _forward_micro_batch_layer_k()
        â”‚   â””â”€ è®¡ç®— Ï€^k_Layer çš„ log_probs
        â”‚   â””â”€ è®¡ç®— PPO loss
        â”‚   â””â”€ åå‘ä¼ æ’­ï¼ˆåªæ›´æ–° 0-k å±‚ï¼‰
        â””â”€ Phase 2: _forward_micro_batch()
            â””â”€ è®¡ç®— Ï€_Î¸ çš„ log_probs
            â””â”€ è®¡ç®— PPO loss
            â””â”€ åå‘ä¼ æ’­ï¼ˆæ›´æ–°æ‰€æœ‰å±‚ï¼‰
    â†“
5. Update Reference Policy (å¯é€‰)
    â””â”€ ref_policy.sync_from_actor()
    â†“
6. Log Metrics & Save Checkpoint
```

### 9.2 meta_info['global_steps'] çš„ç»´æŠ¤

**é—®é¢˜**: `global_steps` æ˜¯å¦‚ä½•ä¼ é€’å’Œæ›´æ–°çš„ï¼Ÿ

**ç­”æ¡ˆ**: åœ¨ RayPPOTrainer ä¸­ç»´æŠ¤

```python
# verl/trainer/ppo/ray_trainer.py (å¤§è‡´)
class RayPPOTrainer:
    def __init__(self, ...):
        self.global_steps = 0

    def train(self):
        for step in range(total_training_steps):
            self.global_steps += 1

            # ç”Ÿæˆ rollout data
            data = self.rollout(...)

            # æ·»åŠ  global_steps åˆ° meta_info
            data.meta_info['global_steps'] = self.global_steps

            # Actor update
            self.actor.update_policy(data)
```

**ä¼ é€’è·¯å¾„**:
```
RayPPOTrainer.global_steps
    â†“ (æ·»åŠ åˆ° data.meta_info)
DataProto.meta_info['global_steps']
    â†“ (split åä¿ç•™)
micro_batch.meta_info['global_steps']
    â†“ (åœ¨ dp_actor.py ä¸­åˆ¤æ–­)
if micro_batch.meta_info['global_steps'] <= iterative_steps:
```

---

## 10. é…ç½®å‚æ•°è¯¦è§£

### 10.1 BuPO ç›¸å…³é…ç½®

```yaml
# verl/trainer/config/actor/dp_actor.yaml
actor:
  internal_policy_interative: False  # å¯ç”¨ BuPO
  internal_layer: 5                   # ä¼˜åŒ–ç¬¬ 5 å±‚
  iterative_steps: 30                 # Phase 1 æŒç»­ 30 æ­¥
```

### 10.2 åœ¨è®­ç»ƒè„šæœ¬ä¸­çš„è®¾ç½®

```bash
# run_code/BuPO_qwen3.sh
k=5                     # å†…éƒ¨å±‚ç´¢å¼•
iterative_steps=30      # Phase 1 æ­¥æ•°

python3 -m verl.trainer.main_ppo \
    actor_rollout_ref.actor.internal_policy_interative=True \
    actor_rollout_ref.actor.internal_layer=${k} \
    actor_rollout_ref.actor.iterative_steps=${iterative_steps} \
    actor_rollout_ref.model.override_config.internal_layer=${k}
```

**å‚æ•°è§£é‡Š**:

| å‚æ•° | å«ä¹‰ | ç¤ºä¾‹å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `internal_policy_interative` | å¯ç”¨ BuPO | True | False è¡¨ç¤ºæ ‡å‡† GRPO |
| `internal_layer` | ä¼˜åŒ–çš„å±‚ç´¢å¼• | 5 | Qwen3-4B æœ‰ 36 å±‚ (0-35) |
| `iterative_steps` | Phase 1 æ­¥æ•° | 30 | å‰ 30 æ­¥ä¼˜åŒ–å†…éƒ¨å±‚ |
| `model.override_config.internal_layer` | æ¨¡å‹é…ç½® | 5 | å‘Šè¯‰æ¨¡å‹è®¡ç®—ç¬¬ 5 å±‚ logits |

### 10.3 å¦‚ä½•é€‰æ‹© internal_layerï¼Ÿ

**è®ºæ–‡å»ºè®®**:

1. **Qwen ç³»åˆ—**:
   - Qwen3-4B (36 å±‚): layer 5 æˆ– 6
   - Qwen3-8B (36 å±‚): layer 5 æˆ– 6
   - **åŸåˆ™**: é€‰æ‹© FFN entropy change å¼€å§‹ä¸º 0 çš„å±‚ï¼ˆIntegration é˜¶æ®µçš„å¼€å§‹ï¼‰

2. **Llama ç³»åˆ—**:
   - Llama-3B (28 å±‚): layer 27ï¼ˆå€’æ•°ç¬¬äºŒå±‚ï¼‰
   - Llama-8B (32 å±‚): layer 31ï¼ˆå€’æ•°ç¬¬äºŒå±‚ï¼‰
   - **åŸåˆ™**: Llama çš„ FFN entropy ä¸€ç›´ä¸ºæ­£ï¼Œé€‰æ‹©æœ€åæœ‰æ­£ entropy çš„å±‚

**æŸ¥çœ‹ entropy åŠ¨æ€**:
```bash
python visualization/plot_internal_entropy.py
# æŸ¥çœ‹ Figure 3 (Entropy Change)ï¼Œé€‰æ‹© Î”H^l_FFN â‰ˆ 0 çš„å±‚
```

---

## 11. å¸¸è§é—®é¢˜ FAQ

### Q1: BuPO ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ

**ç­”æ¡ˆ**:
1. **åº•å±‚ç‰¹å¾ä¼˜å…ˆå¯¹é½**: Phase 1 ä¼˜åŒ–åº•å±‚ï¼Œè®©åº•å±‚å­¦ä¼šé«˜å±‚æ¬¡æ¨ç†
2. **æ¸è¿›å¼è®­ç»ƒ**: å…ˆå¯¹é½åº•å±‚ç‰¹å¾ï¼Œå†å¯¹é½æ•´ä½“ç­–ç•¥
3. **æ›´ç¨³å®šçš„è®­ç»ƒ**: åº•å±‚æä¾›æ›´å¥½çš„ç‰¹å¾åŸºç¡€

### Q2: ä¸ºä»€ä¹ˆéœ€è¦ä¸¤æ¬¡åˆ‡æ¢é€»è¾‘ï¼ˆ806 è¡Œå’Œ 906 è¡Œï¼‰ï¼Ÿ

**ç­”æ¡ˆ**:
- **ç¬¬ 806 è¡Œ** (`compute_ref_log_prob`): è®¡ç®—å‚è€ƒ log_probsï¼ˆæ— æ¢¯åº¦ï¼‰
- **ç¬¬ 906 è¡Œ** (`update_policy`): ç­–ç•¥æ›´æ–°ï¼ˆæœ‰æ¢¯åº¦ï¼‰
- ä¸¤è€…éƒ½éœ€è¦åˆ¤æ–­å½“å‰é˜¶æ®µï¼Œä¿æŒä¸€è‡´

### Q3: internal_layer è®¾ä¸º -1 ä¼šæ€æ ·ï¼Ÿ

**ç­”æ¡ˆ**:
- ä¼šå¯¼è‡´é”™è¯¯ï¼Œå› ä¸º `hidden_states[-1+1]` = `hidden_states[0]` = embedding
- åº”è¯¥è®¾ç½®ä¸ºæœ‰æ•ˆçš„å±‚ç´¢å¼• (0 åˆ° L-1)

### Q4: èƒ½åŒæ—¶ä¼˜åŒ–å¤šä¸ªå†…éƒ¨å±‚å—ï¼Ÿ

**ç­”æ¡ˆ**:
- ä»£ç æ”¯æŒï¼ˆç¬¬ 594 è¡Œçš„ for å¾ªç¯ï¼‰
- ä½†è®ºæ–‡åªä¼˜åŒ–ä¸€å±‚
- å¤šå±‚ä¼˜åŒ–å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š

### Q5: Phase 2 è¿˜èƒ½åˆ‡å› Phase 1 å—ï¼Ÿ

**ç­”æ¡ˆ**:
- ä¸èƒ½ï¼Œè¿™æ˜¯å•å‘çš„
- `global_steps` å•è°ƒé€’å¢ï¼Œä¸€æ—¦è¶…è¿‡ `iterative_steps` å°±ä¸ä¼šå›é€€

### Q6: æ¢¯åº¦è£å‰ªçš„ max_norm=1.0 æ˜¯æ€ä¹ˆç¡®å®šçš„ï¼Ÿ

**ç­”æ¡ˆ**:
- ç»éªŒå€¼ï¼ŒPPO è®­ç»ƒé€šå¸¸ä½¿ç”¨ 1.0
- å¯ä»¥è°ƒæ•´ï¼Œä½†ä¸å»ºè®®è¶…è¿‡ 5.0

---

## 12. è°ƒè¯•æŠ€å·§

### 12.1 æ‰“å° global_steps

```python
# åœ¨ dp_actor.py çš„ç¬¬ 807 è¡Œæ·»åŠ 
print(f"[DEBUG] global_steps={micro_batch.meta_info['global_steps']}, "
      f"iterative_steps={self.config.iterative_steps}, "
      f"Phase={'1' if micro_batch.meta_info['global_steps'] <= self.config.iterative_steps else '2'}")
```

### 12.2 éªŒè¯å†…éƒ¨å±‚ logits

```python
# åœ¨ modeling_qwen3.py çš„ç¬¬ 596 è¡Œæ·»åŠ 
print(f"[DEBUG] internal_layer={startk}, "
      f"mid_layer_logits shape={internal_logits[startk].shape}")
```

### 12.3 æ£€æŸ¥æ¢¯åº¦æµ

```python
# åœ¨ dp_actor.py çš„ç¬¬ 951 è¡Œåæ·»åŠ 
for name, param in self.actor_module.named_parameters():
    if param.grad is not None:
        print(f"[DEBUG] {name}: grad_norm={param.grad.norm().item():.4f}")
```

---

## 13. æ€»ç»“

### 13.1 BuPO æ ¸å¿ƒè¦ç‚¹

1. **Two-Phase Training**:
   - Phase 1: ä¼˜åŒ– Ï€^k_Layer (internal layer policy)
   - Phase 2: ä¼˜åŒ– Ï€Î¸ (full model policy)

2. **å…³é”®å®ç°**:
   - è‡ªå®šä¹‰æ¨¡å‹: è®¡ç®— `mid_layer_logits`
   - ä¸¤é˜¶æ®µåˆ‡æ¢: åˆ¤æ–­ `global_steps <= iterative_steps`
   - æ¢¯åº¦æµæ§åˆ¶: æ®‹å·®è¿æ¥è‡ªåŠ¨é™åˆ¶æ¢¯åº¦èŒƒå›´

3. **ä»£ç ä½ç½®**:
   - è®­ç»ƒå…¥å£: `verl/trainer/main_ppo.py`
   - Actor å®ç°: `verl/workers/actor/dp_actor.py:806-820, 906-920`
   - å†…éƒ¨å±‚å‰å‘: `verl/workers/actor/dp_actor.py:357-555`
   - è‡ªå®šä¹‰æ¨¡å‹: `verl/models/custom_model/modeling_qwen3.py:585-596`

### 13.2 å­¦ä¹ è·¯å¾„å»ºè®®

1. **ç†è§£æ ‡å‡† PPO/GRPO**: å…ˆç†Ÿæ‚‰åŸºç¡€ RL ç®—æ³•
2. **é˜…è¯»è®ºæ–‡ Section 3-5**: ç†è§£ Internal Policy æ¦‚å¿µ
3. **è¿è¡Œå¯è§†åŒ–**: æŸ¥çœ‹ entropy åŠ¨æ€
4. **å•æ­¥è°ƒè¯•**: åœ¨å…³é”®ä½ç½®æ·»åŠ  printï¼Œè§‚å¯Ÿæ•°æ®æµ
5. **ä¿®æ”¹å‚æ•°**: å°è¯•ä¸åŒçš„ `internal_layer` å’Œ `iterative_steps`

### 13.3 æ‰©å±•é˜…è¯»

- **verl æ¡†æ¶æ–‡æ¡£**: https://github.com/volcengine/verl
- **PPO è®ºæ–‡**: https://arxiv.org/abs/1707.06347
- **GRPO è®ºæ–‡**: https://arxiv.org/abs/2402.03300
- **Flash Attention**: https://arxiv.org/abs/2205.14135

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**ä½œè€…**: Claude (Anthropic)
**æœ€åæ›´æ–°**: 2026-01-06
**å¯¹åº”ä»£ç ç‰ˆæœ¬**: BuPO v1.0
**å­—æ•°ç»Ÿè®¡**: ~18,000 å­—
